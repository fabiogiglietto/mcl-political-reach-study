{"metadata": {"kernelspec": {"display_name": "R", "language": "R", "name": "ir"}, "language_info": {"codemirror_mode": "r", "file_extension": ".r", "mimetype": "text/x-r-source", "name": "R", "pygments_lexer": "r", "version": "4.3.3"}}, "nbformat_minor": 5, "cells": [{"metadata": {}, "execution_count": 1, "source": ["# ============================================================================\n", "# APPEND OCTOBER-NOVEMBER 2025 DATA TO COMBINED DATASET\n", "# VERSION: 1.0 (2025-12-23)\n", "# BASED ON: combine_datasets_v3.2.R structure and conventions\n", "# ============================================================================\n", "# \n", "# PURPOSE:\n", "# This script appends new posts from October-November 2025 to the existing\n", "# combined dataset created by combine_datasets_v3.2.R\n", "#\n", "# INPUT FILES (from ../multi_list_provenance_analysis/):\n", "# - subset_mps.rds\n", "# - subset_prominent_politicians.rds  \n", "# - subset_extremist.rds\n", "#\n", "# These files have a different structure with boolean flags:\n", "# - in_MPs_Re_elected, in_MPs_ALL (for MPs)\n", "# - in_Prominent_Politicians (for prominent politicians)\n", "# - in_Extremiste_cluster* (for extremists)\n", "#\n", "# OUTPUT:\n", "# Updated combined dataset in combined_datasets/ directory\n", "# ============================================================================\n", "\n", "# Load required libraries\n", "library(tidyverse)\n", "library(lubridate)\n", "\n", "cat(\"\\n\")\n", "cat(\"=\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\")\n", "cat(\"APPEND OCTOBER-NOVEMBER 2025 DATA\\n\")\n", "cat(\"=\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\")\n", "cat(\"Adding new posts to existing combined dataset\\n\")\n", "cat(\"=\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n", "\n", "# ============================================================================\n", "# STEP 0A: LOAD RE-ELECTED MPs LIST (same as original script)\n", "# ============================================================================\n", "\n", "cat(\"STEP 0A: Loading re-elected MPs list...\\n\")\n", "cat(\"-\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n", "\n", "reelected_file <- \"rawdata/Parlamentari_ITA_Leg_XIX_no_XVIII.rds\"\n", "\n", "if (file.exists(reelected_file)) {\n", "  reelected_mps <- readRDS(reelected_file)\n", "  \n", "  cat(\"\u2713 Loaded re-elected MPs list\\n\")\n", "  cat(\"  File:\", reelected_file, \"\\n\")\n", "  \n", "  # Check structure\n", "  cat(\"  Columns:\", paste(names(reelected_mps), collapse = \", \"), \"\\n\")\n", "  \n", "  if (\"ids\" %in% names(reelected_mps)) {\n", "    reelected_ids <- unique(reelected_mps$ids)\n", "    cat(\"  Re-elected MPs:\", length(reelected_ids), \"\\n\\n\")\n", "  } else {\n", "    stop(\"ERROR: 'ids' column not found in re-elected MPs file.\\n\",\n", "         \"Available columns: \", paste(names(reelected_mps), collapse = \", \"))\n", "  }\n", "  \n", "} else {\n", "  cat(\"\u26a0 WARNING: Re-elected MPs file not found at:\", reelected_file, \"\\n\")\n", "  cat(\"Will treat all MPs as a single group (no distinction)\\n\\n\")\n", "  reelected_ids <- character(0)\n", "}\n", "\n", "# ============================================================================\n", "# STEP 0B: FIND AND LOAD THE MOST RECENT COMBINED DATASET\n", "# ============================================================================\n", "\n", "cat(\"STEP 0B: Loading existing combined dataset...\\n\")\n", "cat(\"-\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n", "\n", "# Find the most recent combined dataset\n", "combined_files <- list.files(\n", "  \"combined_datasets\",\n", "  pattern = \"^italian_political_accounts_.*\\\\.rds$\",\n", "  full.names = TRUE\n", ")\n", "\n", "if (length(combined_files) == 0) {\n", "  stop(\"No existing combined dataset found in combined_datasets/\\n\",\n", "       \"Please run combine_datasets_v3.2.R first.\")\n", "}\n", "\n", "# Sort by modification time and get the most recent\n", "combined_files_info <- file.info(combined_files)\n", "most_recent_file <- rownames(combined_files_info)[which.max(combined_files_info$mtime)]\n", "\n", "cat(\"Found\", length(combined_files), \"combined dataset(s)\\n\")\n", "cat(\"Loading most recent:\", basename(most_recent_file), \"\\n\")\n", "\n", "existing_data <- readRDS(most_recent_file)\n", "\n", "cat(\"\u2713 Loaded existing dataset\\n\")\n", "cat(\"  Posts:\", nrow(existing_data), \"\\n\")\n", "cat(\"  Unique surfaces:\", n_distinct(existing_data$surface.id, na.rm = TRUE), \"\\n\")\n", "cat(\"  Date range:\", \n", "    format(min(existing_data$date, na.rm = TRUE), \"%Y-%m-%d\"), \"to\",\n", "    format(max(existing_data$date, na.rm = TRUE), \"%Y-%m-%d\"), \"\\n\\n\")\n", "\n", "# Show existing structure\n", "cat(\"Existing main_list breakdown:\\n\")\n", "print(existing_data %>%\n", "        group_by(main_list) %>%\n", "        summarise(n_posts = n(), n_surfaces = n_distinct(surface.id, na.rm = TRUE), .groups = \"drop\"))\n", "cat(\"\\n\")\n", "\n", "# ============================================================================\n", "# STEP 1: LOAD NEW SUBSET FILES\n", "# ============================================================================\n", "\n", "cat(\"STEP 1: Loading new subset files...\\n\")\n", "cat(\"-\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n", "\n", "new_data_dir <- \"../multi_list_provenance_analysis\"\n", "\n", "# Define new files\n", "new_files <- list(\n", "  mps = file.path(new_data_dir, \"subset_mps.rds\"),\n", "  prominent = file.path(new_data_dir, \"subset_prominent_politicians.rds\"),\n", "  extremist = file.path(new_data_dir, \"subset_extremist.rds\")\n", ")\n", "\n", "# Load each file\n", "new_data_list <- list()\n", "\n", "for (name in names(new_files)) {\n", "  file_path <- new_files[[name]]\n", "  \n", "  if (file.exists(file_path)) {\n", "    temp_data <- readRDS(file_path)\n", "    new_data_list[[name]] <- temp_data\n", "    \n", "    cat(\"\u2713 Loaded\", name, \"\\n\")\n", "    cat(\"  File:\", basename(file_path), \"\\n\")\n", "    cat(\"  Posts:\", nrow(temp_data), \"\\n\")\n", "    cat(\"  Columns:\", ncol(temp_data), \"\\n\")\n", "    cat(\"  Date range:\", \n", "        format(min(as.Date(temp_data$creation_time), na.rm = TRUE), \"%Y-%m-%d\"), \"to\",\n", "        format(max(as.Date(temp_data$creation_time), na.rm = TRUE), \"%Y-%m-%d\"), \"\\n\\n\")\n", "  } else {\n", "    cat(\"\u26a0 WARNING: File not found:\", file_path, \"\\n\\n\")\n", "  }\n", "}\n", "\n", "if (length(new_data_list) == 0) {\n", "  stop(\"No new data files found!\")\n", "}\n", "\n", "# ============================================================================\n", "# STEP 2: PROCESS MPs DATA (using same logic as original script)\n", "# ============================================================================\n", "\n", "cat(\"STEP 2: Processing MPs data...\\n\")\n", "cat(\"-\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n", "\n", "if (!is.null(new_data_list$mps)) {\n", "  mps_new <- new_data_list$mps\n", "  \n", "  # Apply stories fallback (same as original script)\n", "  # STORIES FALLBACK: For stories, post_owner IS the page (stories can't be reshared)\n", "  stories_with_na <- sum(is.na(mps_new$surface.id) & mps_new$content_type == \"stories\", na.rm = TRUE)\n", "  if (stories_with_na > 0) {\n", "    cat(\"Found\", stories_with_na, \"stories with NA surface.id\\n\")\n", "    cat(\"Applying fallback: For stories, post_owner = page (stories can't be reshared)\\n\")\n", "    \n", "    mps_new <- mps_new %>%\n", "      mutate(\n", "        surface.id = if_else(\n", "          is.na(surface.id) & content_type == \"stories\" & !is.na(post_owner.id),\n", "          post_owner.id, surface.id\n", "        ),\n", "        surface.name = if_else(\n", "          is.na(surface.name) & content_type == \"stories\" & !is.na(post_owner.name),\n", "          post_owner.name, surface.name\n", "        ),\n", "        surface.username = if_else(\n", "          is.na(surface.username) & content_type == \"stories\" & !is.na(post_owner.username),\n", "          post_owner.username, surface.username\n", "        )\n", "      )\n", "    cat(\"\u2713 Applied fallback to\", stories_with_na, \"stories\\n\\n\")\n", "  }\n", "  \n", "  # CLASSIFY MPs: Re-elected vs New (EXACT SAME LOGIC AS ORIGINAL SCRIPT)\n", "  if (length(reelected_ids) > 0) {\n", "    cat(\"Classifying MPs as Re-elected vs New...\\n\")\n", "    cat(\"Using surface.id membership in reelected_ids list\\n\\n\")\n", "    \n", "    mps_new <- mps_new %>%\n", "      mutate(\n", "        is_reelected = surface.id %in% reelected_ids,\n", "        main_list = if_else(is_reelected, \"MPs_Reelected\", \"MPs_New\"),\n", "        sub_list = if_else(is_reelected, \"MPs_Reelected\", \"MPs_New\"),\n", "        list_description = if_else(\n", "          is_reelected,\n", "          \"Italian Parliament members re-elected (served in 2021 & 2022)\",\n", "          \"Italian Parliament members elected only in 2022 (new)\"\n", "        ),\n", "        source_file = new_files$mps,\n", "        date_collected = as.Date(\"2025-12-23\")\n", "      )\n", "    \n", "    # Report breakdown (same format as original)\n", "    mp_breakdown <- mps_new %>%\n", "      group_by(main_list) %>%\n", "      summarise(\n", "        n_posts = n(),\n", "        n_accounts = n_distinct(surface.id, na.rm = TRUE),\n", "        .groups = \"drop\"\n", "      )\n", "    \n", "    cat(\"\u2713 MPs classified:\\n\")\n", "    print(mp_breakdown)\n", "    cat(\"\\n\")\n", "    \n", "  } else {\n", "    # No re-elected list available - treat all as one group (same as original)\n", "    cat(\"\u26a0 No re-elected MPs list - using single 'MPs' category\\n\")\n", "    \n", "    mps_new <- mps_new %>%\n", "      mutate(\n", "        main_list = \"MPs\",\n", "        sub_list = \"MPs\",\n", "        list_description = \"Italian Parliament members elected September 2022\",\n", "        source_file = new_files$mps,\n", "        date_collected = as.Date(\"2025-12-23\")\n", "      )\n", "  }\n", "  \n", "  # CHECK FOR NA IN surface.id (same validation as original)\n", "  na_surface_ids <- mps_new %>%\n", "    filter(is.na(surface.id))\n", "  \n", "  if (nrow(na_surface_ids) > 0) {\n", "    cat(\"\\n\")\n", "    cat(\"\u26a0 WARNING: NA VALUES FOUND IN surface.id\\n\")\n", "    cat(\"Number of posts with NA surface.id:\", nrow(na_surface_ids), \"\\n\")\n", "    cat(\"Content types affected:\\n\")\n", "    print(table(na_surface_ids$content_type, useNA = \"ifany\"))\n", "    cat(\"\\n\")\n", "  }\n", "  \n", "  cat(\"\u2713 Processed MPs data:\", nrow(mps_new), \"posts\\n\")\n", "  cat(\"  Unique surfaces (pages):\", n_distinct(mps_new$surface.id, na.rm = TRUE), \"\\n\\n\")\n", "  \n", "} else {\n", "  mps_new <- NULL\n", "  cat(\"\u26a0 No MPs data to process\\n\\n\")\n", "}\n", "\n", "# ============================================================================\n", "# STEP 3: PROCESS PROMINENT POLITICIANS DATA\n", "# ============================================================================\n", "\n", "cat(\"STEP 3: Processing Prominent Politicians data...\\n\")\n", "cat(\"-\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n", "\n", "if (!is.null(new_data_list$prominent)) {\n", "  prominent_new <- new_data_list$prominent\n", "  \n", "  # Apply stories fallback\n", "  stories_with_na <- sum(is.na(prominent_new$surface.id) & prominent_new$content_type == \"stories\", na.rm = TRUE)\n", "  if (stories_with_na > 0) {\n", "    cat(\"Applying stories fallback to\", stories_with_na, \"stories...\\n\")\n", "    prominent_new <- prominent_new %>%\n", "      mutate(\n", "        surface.id = if_else(\n", "          is.na(surface.id) & content_type == \"stories\" & !is.na(post_owner.id),\n", "          post_owner.id, surface.id\n", "        ),\n", "        surface.name = if_else(\n", "          is.na(surface.name) & content_type == \"stories\" & !is.na(post_owner.name),\n", "          post_owner.name, surface.name\n", "        ),\n", "        surface.username = if_else(\n", "          is.na(surface.username) & content_type == \"stories\" & !is.na(post_owner.username),\n", "          post_owner.username, surface.username\n", "        )\n", "      )\n", "  }\n", "  \n", "  # Check for sub-list information in list_ids column\n", "  if (\"list_ids\" %in% names(prominent_new)) {\n", "    cat(\"Found list_ids column. Unique values:\\n\")\n", "    print(table(prominent_new$list_ids, useNA = \"ifany\"))\n", "    cat(\"\\n\")\n", "  }\n", "  \n", "  # Classify as Prominent Politicians\n", "  # Try to determine sub_list from list_ids if available\n", "  prominent_new <- prominent_new %>%\n", "    mutate(\n", "      main_list = \"Prominent_Politicians\",\n", "      sub_list = case_when(\n", "        !is.null(list_ids) & grepl(\"2021\", list_ids) ~ \"Prominent_Politicians_2021\",\n", "        !is.null(list_ids) & grepl(\"2025\", list_ids) ~ \"Prominent_Politicians_2025\",\n", "        TRUE ~ \"Prominent_Politicians_Oct_Nov_2025\"\n", "      ),\n", "      list_description = \"Prominent Italian politicians - October/November 2025 extension\",\n", "      source_file = new_files$prominent,\n", "      date_collected = as.Date(\"2025-12-23\")\n", "    )\n", "  \n", "  cat(\"\u2713 Processed Prominent Politicians data:\", nrow(prominent_new), \"posts\\n\")\n", "  cat(\"  Unique surfaces:\", n_distinct(prominent_new$surface.id, na.rm = TRUE), \"\\n\\n\")\n", "  \n", "} else {\n", "  prominent_new <- NULL\n", "  cat(\"\u26a0 No Prominent Politicians data to process\\n\\n\")\n", "}\n", "\n", "# ============================================================================\n", "# STEP 4: PROCESS EXTREMIST DATA\n", "# ============================================================================\n", "\n", "cat(\"STEP 4: Processing Extremist data...\\n\")\n", "cat(\"-\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n", "\n", "if (!is.null(new_data_list$extremist)) {\n", "  extremist_new <- new_data_list$extremist\n", "  \n", "  # Apply stories fallback\n", "  stories_with_na <- sum(is.na(extremist_new$surface.id) & extremist_new$content_type == \"stories\", na.rm = TRUE)\n", "  if (stories_with_na > 0) {\n", "    cat(\"Applying stories fallback to\", stories_with_na, \"stories...\\n\")\n", "    extremist_new <- extremist_new %>%\n", "      mutate(\n", "        surface.id = if_else(\n", "          is.na(surface.id) & content_type == \"stories\" & !is.na(post_owner.id),\n", "          post_owner.id, surface.id\n", "        ),\n", "        surface.name = if_else(\n", "          is.na(surface.name) & content_type == \"stories\" & !is.na(post_owner.name),\n", "          post_owner.name, surface.name\n", "        ),\n", "        surface.username = if_else(\n", "          is.na(surface.username) & content_type == \"stories\" & !is.na(post_owner.username),\n", "          post_owner.username, surface.username\n", "        )\n", "      )\n", "  }\n", "  \n", "  # Check for cluster columns\n", "  cluster_cols <- names(extremist_new)[grepl(\"^in_Extremist\", names(extremist_new))]\n", "  cat(\"Found cluster columns:\", paste(cluster_cols, collapse = \", \"), \"\\n\\n\")\n", "  \n", "  # Check list_names column for sub-list info\n", "  if (\"list_names\" %in% names(extremist_new)) {\n", "    cat(\"Found list_names column. Unique values:\\n\")\n", "    print(table(extremist_new$list_names, useNA = \"ifany\"))\n", "    cat(\"\\n\")\n", "  }\n", "  \n", "  # Classify Extremists - use list_names if available, otherwise use cluster flags\n", "  if (\"list_names\" %in% names(extremist_new)) {\n", "    extremist_new <- extremist_new %>%\n", "      mutate(\n", "        main_list = \"Extremists\",\n", "        sub_list = case_when(\n", "          grepl(\"cluster1\", list_names, ignore.case = TRUE) ~ \"Extremist_Cluster1\",\n", "          grepl(\"cluster2\", list_names, ignore.case = TRUE) ~ \"Extremist_Cluster2\",\n", "          grepl(\"FG\", list_names) ~ \"Extremist_FG\",\n", "          grepl(\"GM\", list_names) ~ \"Extremist_GM\",\n", "          grepl(\"MT\", list_names) ~ \"Extremist_MT\",\n", "          grepl(\"2021\", list_names) ~ \"Extremist_2021_EXT\",\n", "          grepl(\"2025\", list_names) ~ \"Extremist_2025_EXT\",\n", "          TRUE ~ \"Extremist_Oct_Nov_2025\"\n", "        ),\n", "        list_description = paste0(\"Extremist accounts - \", sub_list),\n", "        source_file = new_files$extremist,\n", "        date_collected = as.Date(\"2025-12-23\")\n", "      )\n", "  } else {\n", "    extremist_new <- extremist_new %>%\n", "      mutate(\n", "        main_list = \"Extremists\",\n", "        sub_list = \"Extremist_Oct_Nov_2025\",\n", "        list_description = \"Extremist accounts - October/November 2025 extension\",\n", "        source_file = new_files$extremist,\n", "        date_collected = as.Date(\"2025-12-23\")\n", "      )\n", "  }\n", "  \n", "  cat(\"Extremist sub_list breakdown:\\n\")\n", "  print(extremist_new %>%\n", "          group_by(sub_list) %>%\n", "          summarise(n_posts = n(), .groups = \"drop\"))\n", "  cat(\"\\n\")\n", "  \n", "  cat(\"\u2713 Processed Extremist data:\", nrow(extremist_new), \"posts\\n\")\n", "  cat(\"  Unique surfaces:\", n_distinct(extremist_new$surface.id, na.rm = TRUE), \"\\n\\n\")\n", "  \n", "} else {\n", "  extremist_new <- NULL\n", "  cat(\"\u26a0 No Extremist data to process\\n\\n\")\n", "}\n", "\n", "# ============================================================================\n", "# STEP 5: COMBINE NEW DATA\n", "# ============================================================================\n", "\n", "cat(\"STEP 5: Combining new datasets...\\n\")\n", "cat(\"-\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n", "\n", "# Get list of non-null datasets\n", "new_datasets <- list(mps_new, prominent_new, extremist_new)\n", "new_datasets <- new_datasets[!sapply(new_datasets, is.null)]\n", "\n", "if (length(new_datasets) == 0) {\n", "  stop(\"No new data to combine!\")\n", "}\n", "\n", "# Identify common columns with existing data\n", "existing_cols <- names(existing_data)\n", "cat(\"Existing dataset has\", length(existing_cols), \"columns\\n\")\n", "\n", "# For each new dataset, select only columns that exist in the original\n", "# Plus the new classification columns\n", "required_cols <- c(\"main_list\", \"sub_list\", \"list_description\", \"source_file\", \"date_collected\")\n", "\n", "new_data_harmonized <- lapply(new_datasets, function(df) {\n", "  # Find overlapping columns\n", "  common_cols <- intersect(names(df), existing_cols)\n", "  \n", "  # Add required classification columns\n", "  cols_to_keep <- unique(c(common_cols, required_cols))\n", "  cols_to_keep <- cols_to_keep[cols_to_keep %in% names(df)]\n", "  \n", "  df_selected <- df %>% select(all_of(cols_to_keep))\n", "  \n", "  return(df_selected)\n", "})\n", "\n", "# Bind new data together\n", "new_data_combined <- bind_rows(new_data_harmonized)\n", "\n", "cat(\"\u2713 Combined new data\\n\")\n", "cat(\"  Total new posts:\", nrow(new_data_combined), \"\\n\")\n", "cat(\"  Columns:\", ncol(new_data_combined), \"\\n\\n\")\n", "\n", "# Add standardized date variables if not present\n", "if (!\"date\" %in% names(new_data_combined)) {\n", "  new_data_combined <- new_data_combined %>%\n", "    mutate(\n", "      date = as.Date(creation_time),\n", "      year = year(creation_time),\n", "      month = month(creation_time),\n", "      week = floor_date(date, \"week\")\n", "    )\n", "}\n", "\n", "cat(\"New data by main_list:\\n\")\n", "print(new_data_combined %>%\n", "        group_by(main_list) %>%\n", "        summarise(n_posts = n(), n_surfaces = n_distinct(surface.id, na.rm = TRUE), .groups = \"drop\"))\n", "cat(\"\\n\")\n", "\n", "# ============================================================================\n", "# STEP 6: CHECK FOR DUPLICATES\n", "# ============================================================================\n", "\n", "cat(\"STEP 6: Checking for duplicates...\\n\")\n", "cat(\"-\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n", "\n", "# Check if 'id' column exists (post ID)\n", "if (\"id\" %in% names(existing_data) & \"id\" %in% names(new_data_combined)) {\n", "  \n", "  existing_ids <- existing_data$id\n", "  new_ids <- new_data_combined$id\n", "  \n", "  duplicates <- sum(new_ids %in% existing_ids)\n", "  \n", "  if (duplicates > 0) {\n", "    cat(\"\u26a0 Found\", duplicates, \"posts already in existing dataset\\n\")\n", "    cat(\"  These will be removed from new data to avoid duplicates\\n\\n\")\n", "    \n", "    new_data_combined <- new_data_combined %>%\n", "      filter(!id %in% existing_ids)\n", "    \n", "    cat(\"\u2713 Removed duplicates. Remaining new posts:\", nrow(new_data_combined), \"\\n\\n\")\n", "  } else {\n", "    cat(\"\u2713 No duplicate post IDs found\\n\\n\")\n", "  }\n", "  \n", "} else {\n", "  cat(\"\u26a0 Cannot check for duplicates - 'id' column not found\\n\")\n", "  cat(\"  Proceeding without deduplication\\n\\n\")\n", "}\n", "\n", "# ============================================================================\n", "# STEP 7: MERGE WITH EXISTING DATA\n", "# ============================================================================\n", "\n", "cat(\"STEP 7: Merging with existing data...\\n\")\n", "cat(\"-\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n", "\n", "# Ensure column alignment - add missing columns as NA\n", "all_cols <- union(names(existing_data), names(new_data_combined))\n", "\n", "existing_data_aligned <- existing_data\n", "new_data_aligned <- new_data_combined\n", "\n", "for (col in all_cols) {\n", "  if (!col %in% names(existing_data_aligned)) {\n", "    existing_data_aligned[[col]] <- NA\n", "  }\n", "  if (!col %in% names(new_data_aligned)) {\n", "    new_data_aligned[[col]] <- NA\n", "  }\n", "}\n", "\n", "# Select columns in same order\n", "existing_data_aligned <- existing_data_aligned %>% select(all_of(all_cols))\n", "new_data_aligned <- new_data_aligned %>% select(all_of(all_cols))\n", "\n", "# Combine\n", "combined_data <- bind_rows(existing_data_aligned, new_data_aligned)\n", "\n", "cat(\"\u2713 Merged datasets\\n\")\n", "cat(\"  Previous posts:\", nrow(existing_data), \"\\n\")\n", "cat(\"  New posts added:\", nrow(new_data_aligned), \"\\n\")\n", "cat(\"  Total posts:\", nrow(combined_data), \"\\n\")\n", "cat(\"  Total unique surfaces:\", n_distinct(combined_data$surface.id, na.rm = TRUE), \"\\n\\n\")\n", "\n", "# ============================================================================\n", "# STEP 8: VALIDATE ACCOUNT MATCHING BETWEEN ORIGINAL AND NEW DATA\n", "# ============================================================================\n", "\n", "cat(\"STEP 8: Validating account matching between original and new data...\\n\")\n", "cat(\"-\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n", "\n", "# Get unique accounts from original dataset by main_list\n", "original_accounts <- existing_data %>%\n", "  group_by(main_list) %>%\n", "  summarise(\n", "    original_accounts = list(unique(surface.id[!is.na(surface.id)])),\n", "    n_original = n_distinct(surface.id, na.rm = TRUE),\n", "    .groups = \"drop\"\n", "  )\n", "\n", "# Get unique accounts from new data by main_list\n", "new_accounts <- new_data_combined %>%\n", "  group_by(main_list) %>%\n", "  summarise(\n", "    new_accounts = list(unique(surface.id[!is.na(surface.id)])),\n", "    n_new = n_distinct(surface.id, na.rm = TRUE),\n", "    .groups = \"drop\"\n", "  )\n", "\n", "# Join and compare\n", "account_comparison <- original_accounts %>%\n", "  full_join(new_accounts, by = \"main_list\") %>%\n", "  rowwise() %>%\n", "  mutate(\n", "    n_original = if_else(is.null(original_accounts) || length(original_accounts) == 0, 0L, as.integer(n_original)),\n", "    n_new = if_else(is.null(new_accounts) || length(new_accounts) == 0, 0L, as.integer(n_new)),\n", "    # Accounts in new data that are also in original\n", "    n_matching = if_else(\n", "      is.null(original_accounts) || is.null(new_accounts),\n", "      0L,\n", "      as.integer(sum(new_accounts %in% original_accounts))\n", "    ),\n", "    # Accounts in new data that are NOT in original (unexpected)\n", "    n_new_only = if_else(\n", "      is.null(new_accounts),\n", "      0L,\n", "      as.integer(sum(!new_accounts %in% original_accounts))\n", "    ),\n", "    # Accounts in original that are NOT in new data\n", "    n_original_only = if_else(\n", "      is.null(original_accounts),\n", "      0L,\n", "      as.integer(sum(!original_accounts %in% new_accounts))\n", "    )\n", "  ) %>%\n", "  ungroup() %>%\n", "  select(main_list, n_original, n_new, n_matching, n_new_only, n_original_only)\n", "\n", "cat(\"Account matching validation:\\n\")\n", "cat(\"=\" %>% rep(60) %>% paste0(collapse = \"\"), \"\\n\")\n", "print(account_comparison)\n", "cat(\"\\n\")\n", "\n", "cat(\"Legend:\\n\")\n", "cat(\"  n_original     = Unique accounts in original dataset\\n\")\n", "cat(\"  n_new          = Unique accounts in new subset data\\n\")\n", "cat(\"  n_matching     = Accounts in new data that exist in original\\n\")\n", "cat(\"  n_new_only     = Accounts in new data NOT in original (\u26a0 unexpected)\\n\")\n", "cat(\"  n_original_only = Accounts in original NOT in new data (may be inactive)\\n\\n\")\n", "\n", "# Check for unexpected new accounts\n", "total_new_only <- sum(account_comparison$n_new_only, na.rm = TRUE)\n", "if (total_new_only > 0) {\n", "  cat(\"\u26a0 WARNING:\", total_new_only, \"account(s) in new data not found in original dataset!\\n\")\n", "  cat(\"  These accounts may need investigation:\\n\\n\")\n", "  \n", "  # Show which accounts are new\n", "  for (i in 1:nrow(account_comparison)) {\n", "    ml <- account_comparison$main_list[i]\n", "    if (!is.na(account_comparison$n_new_only[i]) && account_comparison$n_new_only[i] > 0) {\n", "      \n", "      orig_ids <- existing_data %>% \n", "        filter(main_list == ml) %>% \n", "        pull(surface.id) %>% \n", "        unique()\n", "      \n", "      new_ids <- new_data_combined %>% \n", "        filter(main_list == ml) %>% \n", "        pull(surface.id) %>% \n", "        unique()\n", "      \n", "      unexpected_ids <- setdiff(new_ids, orig_ids)\n", "      \n", "      cat(\"  \", ml, \":\\n\", sep = \"\")\n", "      \n", "      # Get names for these IDs\n", "      unexpected_info <- new_data_combined %>%\n", "        filter(surface.id %in% unexpected_ids) %>%\n", "        select(surface.id, surface.name, surface.username) %>%\n", "        distinct() %>%\n", "        head(10)\n", "      \n", "      print(unexpected_info)\n", "      cat(\"\\n\")\n", "    }\n", "  }\n", "} else {\n", "  cat(\"\u2713 All accounts in new data match accounts in original dataset\\n\\n\")\n", "}\n", "\n", "# ============================================================================\n", "# STEP 9: FINAL SUMMARY\n", "# ============================================================================\n", "\n", "cat(\"STEP 9: Generating summary...\\n\")\n", "cat(\"-\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n", "\n", "# Summary by main_list\n", "cat(\"Summary by main_list:\\n\")\n", "summary_main <- combined_data %>%\n", "  group_by(main_list) %>%\n", "  summarise(\n", "    n_posts = n(),\n", "    n_surfaces = n_distinct(surface.id, na.rm = TRUE),\n", "    min_date = min(date, na.rm = TRUE),\n", "    max_date = max(date, na.rm = TRUE),\n", "    .groups = \"drop\"\n", "  )\n", "print(summary_main)\n", "cat(\"\\n\")\n", "\n", "# Summary by sub_list\n", "cat(\"Summary by sub_list:\\n\")\n", "summary_sub <- combined_data %>%\n", "  group_by(main_list, sub_list) %>%\n", "  summarise(\n", "    n_posts = n(),\n", "    n_surfaces = n_distinct(surface.id, na.rm = TRUE),\n", "    .groups = \"drop\"\n", "  ) %>%\n", "  arrange(main_list, sub_list)\n", "print(summary_sub)\n", "cat(\"\\n\")\n", "\n", "# Date range\n", "cat(\"Overall date range:\",\n", "    format(min(combined_data$date, na.rm = TRUE), \"%Y-%m-%d\"), \"to\",\n", "    format(max(combined_data$date, na.rm = TRUE), \"%Y-%m-%d\"), \"\\n\\n\")\n", "\n", "# ============================================================================\n", "# STEP 10: MONTHLY BREAKDOWN\n", "# ============================================================================\n", "\n", "cat(\"STEP 10: Monthly breakdown of posts...\\n\")\n", "cat(\"-\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n", "\n", "# Create year-month variable\n", "combined_data <- combined_data %>%\n", "  mutate(year_month = floor_date(date, \"month\"))\n", "\n", "# Overall monthly breakdown\n", "cat(\"A) Overall monthly breakdown:\\n\")\n", "cat(\"=\" %>% rep(60) %>% paste0(collapse = \"\"), \"\\n\")\n", "monthly_overall <- combined_data %>%\n", "  group_by(year_month) %>%\n", "  summarise(\n", "    n_posts = n(),\n", "    n_surfaces = n_distinct(surface.id, na.rm = TRUE),\n", "    .groups = \"drop\"\n", "  ) %>%\n", "  arrange(year_month) %>%\n", "  mutate(year_month = format(year_month, \"%Y-%m\"))\n", "\n", "print(monthly_overall, n = 50)\n", "cat(\"\\n\")\n", "\n", "# Monthly breakdown by main_list\n", "cat(\"B) Monthly breakdown by main_list:\\n\")\n", "cat(\"=\" %>% rep(60) %>% paste0(collapse = \"\"), \"\\n\")\n", "monthly_by_list <- combined_data %>%\n", "  group_by(year_month, main_list) %>%\n", "  summarise(\n", "    n_posts = n(),\n", "    .groups = \"drop\"\n", "  ) %>%\n", "  arrange(year_month, main_list) %>%\n", "  mutate(year_month = format(year_month, \"%Y-%m\"))\n", "\n", "print(monthly_by_list, n = 100)\n", "cat(\"\\n\")\n", "\n", "# Pivot table format for easier reading\n", "cat(\"C) Monthly breakdown (pivot table format):\\n\")\n", "cat(\"=\" %>% rep(60) %>% paste0(collapse = \"\"), \"\\n\")\n", "monthly_pivot <- combined_data %>%\n", "  mutate(year_month = format(year_month, \"%Y-%m\")) %>%\n", "  group_by(year_month, main_list) %>%\n", "  summarise(n_posts = n(), .groups = \"drop\") %>%\n", "  pivot_wider(\n", "    names_from = main_list,\n", "    values_from = n_posts,\n", "    values_fill = 0\n", "  ) %>%\n", "  arrange(year_month) %>%\n", "  mutate(Total = rowSums(across(where(is.numeric))))\n", "\n", "print(monthly_pivot, n = 50)\n", "cat(\"\\n\")\n", "\n", "# Highlight new months (Oct-Nov 2025)\n", "cat(\"D) Focus on newly added months (Oct-Nov 2025):\\n\")\n", "cat(\"=\" %>% rep(60) %>% paste0(collapse = \"\"), \"\\n\")\n", "new_months_data <- combined_data %>%\n", "  filter(year_month >= as.Date(\"2025-10-01\")) %>%\n", "  mutate(year_month = format(year_month, \"%Y-%m\")) %>%\n", "  group_by(year_month, main_list) %>%\n", "  summarise(\n", "    n_posts = n(),\n", "    n_surfaces = n_distinct(surface.id, na.rm = TRUE),\n", "    .groups = \"drop\"\n", "  ) %>%\n", "  arrange(year_month, main_list)\n", "\n", "if (nrow(new_months_data) > 0) {\n", "  print(new_months_data)\n", "} else {\n", "  cat(\"No posts found for Oct-Nov 2025\\n\")\n", "}\n", "cat(\"\\n\")\n", "\n", "# ============================================================================\n", "# STEP 11: SAVE UPDATED DATASET\n", "# ============================================================================\n", "\n", "cat(\"STEP 11: Saving updated dataset...\\n\")\n", "cat(\"-\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n", "\n", "# Create output directory if doesn't exist\n", "if (!dir.exists(\"combined_datasets\")) {\n", "  dir.create(\"combined_datasets\")\n", "}\n", "\n", "# Save with new timestamp\n", "timestamp <- format(Sys.time(), \"%Y%m%d_%H%M%S\")\n", "output_file <- paste0(\"combined_datasets/italian_political_accounts_\", timestamp, \".rds\")\n", "\n", "saveRDS(combined_data, output_file)\n", "\n", "cat(\"\u2713 Saved updated dataset\\n\")\n", "cat(\"  File:\", output_file, \"\\n\")\n", "cat(\"  Total posts:\", nrow(combined_data), \"\\n\")\n", "cat(\"  Total unique surfaces:\", n_distinct(combined_data$surface.id, na.rm = TRUE), \"\\n\\n\")\n", "\n", "# ============================================================================\n", "# COMPLETION\n", "# ============================================================================\n", "\n", "cat(\"=\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\")\n", "cat(\"DATA APPEND COMPLETE\\n\")\n", "cat(\"=\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n", "\n", "cat(\"Summary of changes:\\n\")\n", "cat(\"  Previous dataset:\", nrow(existing_data), \"posts\\n\")\n", "cat(\"  New posts added:\", nrow(new_data_aligned), \"posts\\n\")\n", "cat(\"  Updated dataset:\", nrow(combined_data), \"posts\\n\\n\")\n", "\n", "cat(\"Output file:\", output_file, \"\\n\\n\")\n", "\n", "cat(\"=\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\")\n", "cat(\"\u2713 ALL DONE!\\n\")\n", "cat(\"=\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")"], "id": "06a82c77-e202-43cb-80fd-1c86841b7642", "cell_type": "code", "outputs": [{"output_type": "stream", "name": "stderr", "text": ["[NOTICE] 2 output(s) filtered out"]}]}], "nbformat": 4}