{
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat_minor": 5,
 "cells": [
  {
   "metadata": {},
   "execution_count": 1,
   "source": [
    "# =============================================================================\n",
    "# DATA VALIDATION AND CLEANING SCRIPT v3.2 (GROUP-SPECIFIC IMPUTATION)\n",
    "# Meta Political Content Research - Italian Parliamentarians Dataset\n",
    "# =============================================================================\n",
    "#\n",
    "# This script validates, cleans, and prepares the combined Italian political\n",
    "# accounts dataset for analysis. It ensures data integrity after multiple\n",
    "# merges and creates analysis-ready datasets for RQ1-RQ4.\n",
    "#\n",
    "# CHANGE FROM v3.2: Group-specific view imputation method\n",
    "#   - OLD (v3.1): Pooled ratio-weighted (ratio=1.31 for all groups)\n",
    "#   - NEW (v3.2): Group-specific ratios derived from each group's distribution\n",
    "#   - Parameters derived dynamically via power law fit to 101-500 view range\n",
    "#   - Addresses heterogeneity: Extremists more left-skewed, Prominent more right-skewed\n",
    "#   - Sensitivity: Maximum 0.002% difference between pooled and group-specific\n",
    "#\n",
    "# MODIFICATION: Complete Weeks Only\n",
    "#   - Weekly aggregation now only includes complete weeks\n",
    "#   - Partial weeks at the start and end of the study period are excluded\n",
    "#   - This ensures consistent temporal comparison across all groups\n",
    "#\n",
    "# STUDY TIME-FRAME:\n",
    "#   - Start date: 2021-01-01 (Pre-policy baseline)\n",
    "#   - End date:   2025-11-30 (Post-reversal period)\n",
    "#   - Posts outside this range are excluded from analysis\n",
    "#\n",
    "# CRITICAL CHECKS PERFORMED:\n",
    "#   1. Merge integrity: Each surface.id assigned to exactly ONE main_list\n",
    "#   2. Duplicate detection: No duplicate post IDs in dataset\n",
    "#   3. Metadata consistency: Account info consistent across posts\n",
    "#   4. Temporal filtering: Posts within study time-frame only\n",
    "#   5. NA handling: Views imputed, engagement NAs removed\n",
    "#\n",
    "# INPUT:\n",
    "#   - Combined dataset from combine_datasets_*.R scripts\n",
    "#   - Location: combined_datasets/political_posts_*.rds\n",
    "#   - Compatible with v2 (3 groups) and v3.2 (4 groups with MP split)\n",
    "#\n",
    "# OUTPUT DATASETS (saved to cleaned_data/):\n",
    "#   1. cleaned_posts_*.rds       - All posts with analysis variables\n",
    "#   2. accounts_summary_*.rds    - Account-level aggregated statistics\n",
    "#   3. accounts_both_periods_*.rds - Accounts active pre AND post policy\n",
    "#   4. posts_both_periods_*.rds  - Posts from balanced panel accounts\n",
    "#   5. weekly_aggregation_*.rds  - Weekly time series for RQ1 (COMPLETE WEEKS ONLY)\n",
    "#   6. monthly_aggregation_*.rds - Monthly time series for RQ1\n",
    "#   7. account_period_*.rds      - Account-level by period (pre/post)\n",
    "#   8. surface_info_*.rds/csv    - Comprehensive account metadata\n",
    "#\n",
    "# OUTPUT FOR MCL API QUERIES (to fetch additional account info):\n",
    "#   - surface_ids_for_api_*.txt  - One ID per line\n",
    "#   - surface_ids_for_api_*.csv  - IDs with names and list assignments\n",
    "#   - surface_ids_r_vector_*.R   - Copy-paste ready R vector\n",
    "#   - surface_ids_[LIST]_*.txt   - Separate files by main_list\n",
    "#\n",
    "# NA HANDLING STRATEGY:\n",
    "#   - Views (NA):        Imputed with GROUP-SPECIFIC ratio-weighted method\n",
    "#                        (parameters derived dynamically from each group's distribution)\n",
    "#   - Reactions (NA):    Posts REMOVED - no documented threshold\n",
    "#   - Shares (NA):       Posts REMOVED - no documented threshold  \n",
    "#   - Comments (NA):     Posts REMOVED - no documented threshold\n",
    "#   - Content_type (NA): Labeled 'mcl_unsupported_attachment' - MCL does not\n",
    "#                        support this attachment type\n",
    "#\n",
    "# MERGE INTEGRITY RESOLUTION OPTIONS:\n",
    "#   - STOP (default): Halt execution if issues found, save diagnostics\n",
    "#   - REMOVE:         Remove all posts from affected accounts\n",
    "#   - MAJORITY:       Assign account to list with most posts\n",
    "#   - PRIORITY:       Use priority order (MPs > Prominent > Extremists)\n",
    "#\n",
    "# RESEARCH QUESTIONS SUPPORTED:\n",
    "#   - RQ1: When/extent of reach reduction → weekly/monthly_aggregation\n",
    "#   - RQ2: Engagement-reach moderation   → cleaned_posts\n",
    "#   - RQ3: Equal effects across spectrum → accounts_both_periods\n",
    "#   - RQ4: Experience effects (v3.2)     → accounts_both_periods (MPs only)\n",
    "#\n",
    "# REQUIREMENTS:\n",
    "#   - R packages: tidyverse, lubridate\n",
    "#   - Input: Combined dataset from combine_datasets_*.R\n",
    "#\n",
    "# USAGE:\n",
    "#   1. Ensure combined dataset exists in combined_datasets/\n",
    "#   2. (Optional) Modify study_start_date / study_end_date if needed\n",
    "#   3. (Optional) Modify multi_list_resolution if integrity issues expected\n",
    "#   4. Run script: source(\"data_validation_cleaning_v3.R\")\n",
    "#   5. Check console output for validation results\n",
    "#   6. Load cleaned data: readRDS(\"cleaned_data/cleaned_posts_*.rds\")\n",
    "#\n",
    "# VERSION HISTORY:\n",
    "#   v1.0 - Initial validation script\n",
    "#   v2.0 - Added NA removal (instead of imputation) for engagement metrics\n",
    "#   v3.0 - Added comprehensive merge integrity checks\n",
    "#        - Added surface info dataset with account metadata\n",
    "#        - Added MCL API query file outputs\n",
    "#        - Added automatic version detection (3 vs 4 groups)\n",
    "#   v3.1 - Added study time-frame filtering (2021-01-01 to 2025-11-30)\n",
    "#   v3.1-REFINED - Updated view imputation to ratio-weighted method\n",
    "#                  based on empirical power law extrapolation (pooled ratio=1.31)\n",
    "#   v3.2 - GROUP-SPECIFIC imputation: derives separate parameters for each group\n",
    "#        - Changed default conflict resolution to PRIORITY\n",
    "#        - Parameters: Extremists (α=-0.36, ratio=1.65), Prominent (α=0.33, ratio=0.66),\n",
    "#          MPs_Reelected (α=0.14, ratio=0.82), MPs_New (α=0.03, ratio=0.94)\n",
    "#   v3.2-COMPLETE-WEEKS - Added complete weeks filtering for weekly aggregation\n",
    "#        - Partial weeks at start/end of study period are excluded\n",
    "#        - Ensures consistent temporal comparison\n",
    "#\n",
    "# AUTHOR: [Your name]\n",
    "# DATE: [Current date]\n",
    "# PROJECT: Meta Political Content Moderation Research\n",
    "# =============================================================================\n",
    "\n",
    "library(tidyverse)\n",
    "library(lubridate)\n",
    "\n",
    "# Load shared utilities and configuration\n",
    "source(\"scripts/utils.R\")\n",
    "config <- load_config(\"IT\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# ┌─────────────────────────────────────────────────────────────────────────┐\n",
    "# │ STUDY TIME-FRAME                                                        │\n",
    "# │ Posts outside this range will be excluded from analysis                 │\n",
    "# └─────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "study_start_date <- config$study_period$start_date  # Pre-policy baseline\n",
    "study_end_date   <- config$study_period$end_date    # Post-reversal period\n",
    "\n",
    "# ┌─────────────────────────────────────────────────────────────────────────┐\n",
    "# │ COMPLETE WEEKS CONFIGURATION                                            │\n",
    "# │ Only complete weeks will be included in weekly_aggregation              │\n",
    "# └─────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "# Week start day (1 = Monday, 7 = Sunday) - using ISO standard (Monday)\n",
    "# lubridate floor_date with \"week\" uses weeks starting on Sunday by default\n",
    "# We'll calculate complete weeks based on the actual week boundaries\n",
    "\n",
    "# ┌─────────────────────────────────────────────────────────────────────────┐\n",
    "# │ GROUP-SPECIFIC IMPUTATION CONFIGURATION                                 │\n",
    "# │ Parameters derived dynamically from each group's distribution           │\n",
    "# └─────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "IMPUTATION_BIN_WIDTH <- 50      # Bin width for power law fitting\n",
    "IMPUTATION_MAX_VIEW <- 500      # Maximum view count for fitting\n",
    "IMPUTATION_MIN_BINS <- 3        # Minimum non-empty bins for reliable fit\n",
    "IMPUTATION_MIN_POSTS <- 100     # Minimum posts near threshold for fitting\n",
    "POOLED_FALLBACK_RATIO <- 1.0    # Fallback ratio if group fit fails (uniform)\n",
    "\n",
    "# =============================================================================\n",
    "# HELPER FUNCTIONS FOR COMPLETE WEEKS\n",
    "# =============================================================================\n",
    "\n",
    "#' Calculate the first complete week start date\n",
    "#' \n",
    "#' Given a study start date, returns the start of the first complete week.\n",
    "#' If the study starts on a Sunday (week start), that date is returned.\n",
    "#' Otherwise, returns the following Sunday.\n",
    "#' \n",
    "#' @param start_date The study start date\n",
    "#' @return The start date of the first complete week\n",
    "#' \n",
    "get_first_complete_week_start <- function(start_date) {\n",
    "  # floor_date gives the start of the week containing the date (Sunday)\n",
    "  week_start <- floor_date(start_date, \"week\")\n",
    "  \n",
    "  # If study starts exactly on a Sunday, that week is complete from our perspective\n",
    "  if (week_start == start_date) {\n",
    "    return(start_date)\n",
    "  } else {\n",
    "    # Otherwise, the first complete week starts next Sunday\n",
    "    return(week_start + days(7))\n",
    "  }\n",
    "}\n",
    "\n",
    "#' Calculate the last complete week end date\n",
    "#' \n",
    "#' Given a study end date, returns the end (Saturday) of the last complete week.\n",
    "#' A complete week ends on Saturday (day before Sunday).\n",
    "#' \n",
    "#' @param end_date The study end date\n",
    "#' @return The end date of the last complete week (Saturday)\n",
    "#' \n",
    "get_last_complete_week_end <- function(end_date) {\n",
    "  # Get the start of the week containing end_date\n",
    "  week_start <- floor_date(end_date, \"week\")\n",
    "  \n",
    "  # The end of that week is Saturday (week_start + 6 days)\n",
    "  week_end <- week_start + days(6)\n",
    "  \n",
    "  # If end_date is Saturday or later in the week, that week is complete\n",
    "  if (end_date >= week_end) {\n",
    "    return(week_end)\n",
    "  } else {\n",
    "    # Otherwise, the last complete week is the previous week\n",
    "    return(week_end - days(7))\n",
    "  }\n",
    "}\n",
    "\n",
    "#' Check if a week is complete within the study period\n",
    "#' \n",
    "#' @param week_start The start date of the week (Sunday)\n",
    "#' @param study_start The study start date\n",
    "#' @param study_end The study end date\n",
    "#' @return TRUE if the week is complete, FALSE otherwise\n",
    "#' \n",
    "is_complete_week <- function(week_start, study_start, study_end) {\n",
    "  week_end <- week_start + days(6)  # Saturday\n",
    "  return(week_start >= study_start & week_end <= study_end)\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# GROUP-SPECIFIC IMPUTATION FUNCTIONS (v3.2)\n",
    "# =============================================================================\n",
    "# Each group's imputation parameters are derived from its own near-threshold\n",
    "# distribution via power law extrapolation. This addresses heterogeneity:\n",
    "#   - Extremists: More left-skewed (ratio ~1.65, more very low views)\n",
    "#   - Prominent Politicians: More right-skewed (ratio ~0.66, closer to 100)\n",
    "#   - MPs: Between these extremes (ratio ~0.82-0.94)\n",
    "# =============================================================================\n",
    "\n",
    "#' Derive imputation ratio for a group from near-threshold distribution\n",
    "#' \n",
    "#' Fits power law to bin counts and extrapolates to estimate ratio of\n",
    "#' posts in 1-50 vs 50-100 range.\n",
    "#' \n",
    "#' @param views Vector of observed (non-NA) views for the group\n",
    "#' @param bin_width Width of bins for fitting (default 50)\n",
    "#' @param max_view Maximum view count to include in fit (default 500)\n",
    "#' @param min_bins Minimum non-empty bins required (default 3)\n",
    "#' @param min_posts Minimum posts near threshold required (default 100)\n",
    "#' @param fallback_ratio Ratio to use if fit fails (default 1.0 = uniform)\n",
    "#' @return List with ratio, alpha, r_squared, and reliability flag\n",
    "#' \n",
    "derive_imputation_ratio <- function(views, \n",
    "                                    bin_width = IMPUTATION_BIN_WIDTH, \n",
    "                                    max_view = IMPUTATION_MAX_VIEW,\n",
    "                                    min_bins = IMPUTATION_MIN_BINS,\n",
    "                                    min_posts = IMPUTATION_MIN_POSTS,\n",
    "                                    fallback_ratio = POOLED_FALLBACK_RATIO) {\n",
    "  \n",
    "  # Filter to near-threshold views (101 to max_view)\n",
    "  near_threshold <- views[views > 100 & views <= max_view]\n",
    "  n_near <- length(near_threshold)\n",
    "  \n",
    "  # Check if enough data\n",
    "  if (n_near < min_posts) {\n",
    "    return(list(\n",
    "      ratio = fallback_ratio,\n",
    "      alpha = NA,\n",
    "      r_squared = NA,\n",
    "      n_near_threshold = n_near,\n",
    "      reliable = FALSE,\n",
    "      reason = paste(\"Insufficient posts near threshold:\", n_near, \"<\", min_posts)\n",
    "    ))\n",
    "  }\n",
    "  \n",
    "  # Create bins\n",
    "  breaks <- seq(100, max_view, by = bin_width)\n",
    "  bins <- cut(near_threshold, breaks = breaks, include.lowest = TRUE)\n",
    "  bin_counts <- table(bins)\n",
    "  bin_mids <- breaks[-length(breaks)] + bin_width / 2\n",
    "  \n",
    "  # Filter out zero counts\n",
    "  valid <- bin_counts > 0\n",
    "  n_valid_bins <- sum(valid)\n",
    "  \n",
    "  if (n_valid_bins < min_bins) {\n",
    "    return(list(\n",
    "      ratio = fallback_ratio,\n",
    "      alpha = NA,\n",
    "      r_squared = NA,\n",
    "      n_near_threshold = n_near,\n",
    "      reliable = FALSE,\n",
    "      reason = paste(\"Insufficient non-empty bins:\", n_valid_bins, \"<\", min_bins)\n",
    "    ))\n",
    "  }\n",
    "  \n",
    "  # Fit power law: log(count) ~ log(bin_mid)\n",
    "  fit <- lm(log(as.numeric(bin_counts[valid])) ~ log(bin_mids[valid]))\n",
    "  alpha <- coef(fit)[2]\n",
    "  r_squared <- summary(fit)$r.squared\n",
    "  \n",
    "  # Derive ratio using integral of power law\n",
    "  a <- alpha + 1\n",
    "  \n",
    "  if (abs(a) > 1e-10) {\n",
    "    integral_1_50 <- (50^a - 1^a) / a\n",
    "    integral_50_100 <- (100^a - 50^a) / a\n",
    "  } else {\n",
    "    integral_1_50 <- log(50) - log(1)\n",
    "    integral_50_100 <- log(100) - log(50)\n",
    "  }\n",
    "  \n",
    "  ratio <- integral_1_50 / integral_50_100\n",
    "  \n",
    "  # Ensure ratio is positive and reasonable\n",
    "  if (is.na(ratio) || ratio <= 0 || ratio > 10) {\n",
    "    return(list(\n",
    "      ratio = fallback_ratio,\n",
    "      alpha = alpha,\n",
    "      r_squared = r_squared,\n",
    "      n_near_threshold = n_near,\n",
    "      reliable = FALSE,\n",
    "      reason = paste(\"Derived ratio out of range:\", round(ratio, 3))\n",
    "    ))\n",
    "  }\n",
    "  \n",
    "  return(list(\n",
    "    ratio = ratio,\n",
    "    alpha = alpha,\n",
    "    r_squared = r_squared,\n",
    "    n_near_threshold = n_near,\n",
    "    reliable = TRUE,\n",
    "    reason = \"Successfully derived from data\"\n",
    "  ))\n",
    "}\n",
    "\n",
    "#' Generate imputed values using ratio-weighted method\n",
    "#' \n",
    "#' @param n Number of values to generate\n",
    "#' @param ratio Ratio of posts in 1-50 vs 50-100 (>1 means more low values)\n",
    "#' @param seed Random seed (NULL for no seed setting within function)\n",
    "#' @return Vector of imputed integer values in [1, 100]\n",
    "#' \n",
    "generate_imputed_values <- function(n, ratio, seed = NULL) {\n",
    "  if (!is.null(seed)) set.seed(seed)\n",
    "  \n",
    "  if (n == 0) return(integer(0))\n",
    "  \n",
    "  p_low <- ratio / (ratio + 1)\n",
    "  in_low_half <- runif(n) < p_low\n",
    "  \n",
    "  values <- integer(n)\n",
    "  n_low <- sum(in_low_half)\n",
    "  n_high <- n - n_low\n",
    "  \n",
    "  if (n_low > 0) {\n",
    "    values[in_low_half] <- sample(1:50, n_low, replace = TRUE)\n",
    "  }\n",
    "  if (n_high > 0) {\n",
    "    values[!in_low_half] <- sample(51:100, n_high, replace = TRUE)\n",
    "  }\n",
    "  \n",
    "  return(values)\n",
    "}\n",
    "\n",
    "#' Compute summary statistics for imputed values\n",
    "#' \n",
    "#' @param ratio Ratio used for imputation\n",
    "#' @param n_samples Number of samples for estimation (default 100000)\n",
    "#' @return List with mean, median, sd, pct_under_50\n",
    "#' \n",
    "compute_imputation_stats <- function(ratio, n_samples = 100000) {\n",
    "  set.seed(12345)  # Fixed seed for consistent stats reporting\n",
    "  values <- generate_imputed_values(n_samples, ratio)\n",
    "  \n",
    "  list(\n",
    "    mean = round(mean(values), 1),\n",
    "    median = median(values),\n",
    "    sd = round(sd(values), 1),\n",
    "    pct_under_50 = round(100 * mean(values <= 50), 1),\n",
    "    p_low = round(100 * ratio / (ratio + 1), 1)\n",
    "  )\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "cat(\"\\n\")\n",
    "cat(\"=\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\")\n",
    "cat(\"DATA VALIDATION AND CLEANING (v3.2 - GROUP-SPECIFIC IMPUTATION)\\n\")\n",
    "cat(\"WITH COMPLETE WEEKS FILTERING\\n\")\n",
    "cat(\"=\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: LOAD DATA\n",
    "# ============================================================================\n",
    "\n",
    "cat(\"STEP 1: Loading combined dataset...\\n\")\n",
    "cat(\"-\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n",
    "\n",
    "# Find most recent combined dataset\n",
    "# Find and load most recent combined dataset\n",
    "data_file <- find_most_recent_file(\"combined_datasets\", \"political_posts_.*\\\\.rds$\")\n",
    "\n",
    "if (is.null(data_file)) {\n",
    "  stop(\"No combined dataset found in combined_datasets/ directory.\\n\",\n",
    "       \"Please run the dataset combination script first.\")\n",
    "}\n",
    "\n",
    "cat(\"Loading:\", basename(data_file), \"\\n\")\n",
    "cat(\"File date:\", format(file.mtime(data_file), \"%Y-%m-%d %H:%M:%S\"), \"\\n\\n\")\n",
    "\n",
    "data <- readRDS(data_file)\n",
    "\n",
    "cat(\"✓ Loaded data\\n\")\n",
    "cat(\"  Total posts:\", nrow(data), \"\\n\")\n",
    "cat(\"  Columns:\", ncol(data), \"\\n\")\n",
    "cat(\"  Unique surface.id:\", n_distinct(data$surface.id), \"\\n\")\n",
    "cat(\"  Unique post id:\", n_distinct(data$id), \"\\n\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: MERGE INTEGRITY VALIDATION (CRITICAL)\n",
    "# ============================================================================\n",
    "\n",
    "cat(\"STEP 2: Validating merge integrity (CRITICAL CHECKS)...\\n\")\n",
    "cat(\"-\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n",
    "\n",
    "integrity_errors <- list()\n",
    "integrity_warnings <- list()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# CHECK 2.1: Each surface.id must be assigned to exactly ONE main_list\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "cat(\"CHECK 2.1: Verifying each account (surface.id) has exactly ONE main_list...\\n\\n\")\n",
    "\n",
    "surface_list_mapping <- data %>%\n",
    "  group_by(surface.id) %>%\n",
    "  summarise(\n",
    "    n_lists = n_distinct(main_list),\n",
    "    lists = paste(sort(unique(main_list)), collapse = \", \"),\n",
    "    n_posts = n(),\n",
    "    surface_name = first(surface.name),\n",
    "    surface_username = first(surface.username),\n",
    "    .groups = \"drop\"\n",
    "  )\n",
    "\n",
    "# Accounts with multiple list assignments\n",
    "multi_list_accounts <- surface_list_mapping %>%\n",
    "  filter(n_lists > 1) %>%\n",
    "  arrange(desc(n_lists), desc(n_posts))\n",
    "\n",
    "if (nrow(multi_list_accounts) > 0) {\n",
    "  cat(\"✗ CRITICAL ERROR: Found\", nrow(multi_list_accounts), \n",
    "      \"accounts assigned to MULTIPLE lists!\\n\\n\")\n",
    "  \n",
    "  cat(\"Accounts with multiple list assignments:\\n\")\n",
    "  cat(\"-\" %>% rep(60) %>% paste0(collapse = \"\"), \"\\n\")\n",
    "  \n",
    "  # Print details\n",
    "  for (i in 1:min(nrow(multi_list_accounts), 50)) {\n",
    "    acc <- multi_list_accounts[i, ]\n",
    "    cat(sprintf(\"\\n  Account %d:\\n\", i))\n",
    "    cat(sprintf(\"    surface.id: %s\\n\", acc$surface.id))\n",
    "    cat(sprintf(\"    name: %s\\n\", acc$surface_name))\n",
    "    cat(sprintf(\"    username: %s\\n\", acc$surface_username))\n",
    "    cat(sprintf(\"    assigned to: %s\\n\", acc$lists))\n",
    "    cat(sprintf(\"    total posts: %d\\n\", acc$n_posts))\n",
    "    \n",
    "    # Show post distribution across lists for this account\n",
    "    post_dist <- data %>%\n",
    "      filter(surface.id == acc$surface.id) %>%\n",
    "      count(main_list) %>%\n",
    "      mutate(info = sprintf(\"%s: %d posts\", main_list, n))\n",
    "    \n",
    "    cat(sprintf(\"    distribution: %s\\n\", paste(post_dist$info, collapse = \", \")))\n",
    "  }\n",
    "  \n",
    "  if (nrow(multi_list_accounts) > 50) {\n",
    "    cat(\"\\n  ... and\", nrow(multi_list_accounts) - 50, \"more accounts\\n\")\n",
    "  }\n",
    "  \n",
    "  cat(\"\\n\")\n",
    "  \n",
    "  # Store for later handling\n",
    "  integrity_errors$multi_list_accounts <- multi_list_accounts\n",
    "  \n",
    "  # Calculate total affected posts\n",
    "  affected_posts <- data %>%\n",
    "    filter(surface.id %in% multi_list_accounts$surface.id) %>%\n",
    "    nrow()\n",
    "  \n",
    "  cat(\"Total posts affected:\", affected_posts, \"\\n\\n\")\n",
    "  \n",
    "} else {\n",
    "  cat(\"✓ PASSED: All\", nrow(surface_list_mapping), \n",
    "      \"accounts are assigned to exactly ONE list\\n\\n\")\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# CHECK 2.2: Duplicate post IDs\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "cat(\"CHECK 2.2: Checking for duplicate post IDs...\\n\\n\")\n",
    "\n",
    "duplicate_posts <- data %>%\n",
    "  group_by(id) %>%\n",
    "  filter(n() > 1) %>%\n",
    "  ungroup() %>%\n",
    "  arrange(id)\n",
    "\n",
    "if (nrow(duplicate_posts) > 0) {\n",
    "  n_dup_ids <- n_distinct(duplicate_posts$id)\n",
    "  cat(\"✗ ERROR: Found\", nrow(duplicate_posts), \"rows with\", n_dup_ids, \"duplicate post IDs!\\n\\n\")\n",
    "  \n",
    "  # Show examples\n",
    "  cat(\"Examples of duplicate posts:\\n\")\n",
    "  dup_examples <- duplicate_posts %>%\n",
    "    group_by(id) %>%\n",
    "    slice_head(n = 2) %>%\n",
    "    ungroup() %>%\n",
    "    head(20) %>%\n",
    "    dplyr::select(id, surface.id, surface.name, main_list, date, statistics.views)\n",
    "  \n",
    "  print(dup_examples)\n",
    "  cat(\"\\n\")\n",
    "  \n",
    "  integrity_errors$duplicate_posts <- duplicate_posts\n",
    "  \n",
    "} else {\n",
    "  cat(\"✓ PASSED: All\", n_distinct(data$id), \"post IDs are unique\\n\\n\")\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# CHECK 2.3: Consistency of account metadata across posts\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "cat(\"CHECK 2.3: Checking consistency of account metadata across posts...\\n\\n\")\n",
    "\n",
    "# Check if surface.name and surface.username are consistent for each surface.id\n",
    "metadata_consistency <- data %>%\n",
    "  group_by(surface.id) %>%\n",
    "  summarise(\n",
    "    n_names = n_distinct(surface.name, na.rm = TRUE),\n",
    "    n_usernames = n_distinct(surface.username, na.rm = TRUE),\n",
    "    names = paste(unique(na.omit(surface.name)), collapse = \" | \"),\n",
    "    usernames = paste(unique(na.omit(surface.username)), collapse = \" | \"),\n",
    "    .groups = \"drop\"\n",
    "  )\n",
    "\n",
    "inconsistent_names <- metadata_consistency %>%\n",
    "  filter(n_names > 1)\n",
    "\n",
    "inconsistent_usernames <- metadata_consistency %>%\n",
    "  filter(n_usernames > 1)\n",
    "\n",
    "if (nrow(inconsistent_names) > 0) {\n",
    "  cat(\"⚠ WARNING: Found\", nrow(inconsistent_names), \n",
    "      \"accounts with inconsistent surface.name values\\n\")\n",
    "  cat(\"  (This may be due to name changes over time - usually not critical)\\n\")\n",
    "  \n",
    "  if (nrow(inconsistent_names) <= 10) {\n",
    "    print(inconsistent_names %>% dplyr::select(surface.id, n_names, names))\n",
    "  } else {\n",
    "    print(head(inconsistent_names %>% dplyr::select(surface.id, n_names, names), 10))\n",
    "    cat(\"  ... and\", nrow(inconsistent_names) - 10, \"more\\n\")\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "  \n",
    "  integrity_warnings$inconsistent_names <- inconsistent_names\n",
    "} else {\n",
    "  cat(\"✓ PASSED: surface.name is consistent for all accounts\\n\")\n",
    "}\n",
    "\n",
    "if (nrow(inconsistent_usernames) > 0) {\n",
    "  cat(\"⚠ WARNING: Found\", nrow(inconsistent_usernames), \n",
    "      \"accounts with inconsistent surface.username values\\n\")\n",
    "  \n",
    "  if (nrow(inconsistent_usernames) <= 10) {\n",
    "    print(inconsistent_usernames %>% dplyr::select(surface.id, n_usernames, usernames))\n",
    "  } else {\n",
    "    print(head(inconsistent_usernames %>% dplyr::select(surface.id, n_usernames, usernames), 10))\n",
    "    cat(\"  ... and\", nrow(inconsistent_usernames) - 10, \"more\\n\")\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "  \n",
    "  integrity_warnings$inconsistent_usernames <- inconsistent_usernames\n",
    "} else {\n",
    "  cat(\"✓ PASSED: surface.username is consistent for all accounts\\n\")\n",
    "}\n",
    "cat(\"\\n\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# CHECK 2.4: Sub-list consistency within main_list\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "cat(\"CHECK 2.4: Checking sub_list consistency within main_list...\\n\\n\")\n",
    "\n",
    "if (\"sub_list\" %in% names(data)) {\n",
    "  sublist_consistency <- data %>%\n",
    "    group_by(surface.id, main_list) %>%\n",
    "    summarise(\n",
    "      n_sublists = n_distinct(sub_list, na.rm = TRUE),\n",
    "      sublists = paste(unique(na.omit(sub_list)), collapse = \" | \"),\n",
    "      .groups = \"drop\"\n",
    "    ) %>%\n",
    "    filter(n_sublists > 1)\n",
    "  \n",
    "  if (nrow(sublist_consistency) > 0) {\n",
    "    cat(\"⚠ WARNING: Found\", nrow(sublist_consistency), \n",
    "        \"accounts with inconsistent sub_list within main_list\\n\")\n",
    "    print(head(sublist_consistency, 10))\n",
    "    cat(\"\\n\")\n",
    "    \n",
    "    integrity_warnings$inconsistent_sublists <- sublist_consistency\n",
    "  } else {\n",
    "    cat(\"✓ PASSED: sub_list is consistent within main_list for all accounts\\n\\n\")\n",
    "  }\n",
    "} else {\n",
    "  cat(\"ℹ sub_list column not found - skipping check\\n\\n\")\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# CHECK 2.5: Cross-reference post counts with unique accounts\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "cat(\"CHECK 2.5: Cross-referencing counts...\\n\\n\")\n",
    "\n",
    "# Summary by main_list\n",
    "list_summary <- data %>%\n",
    "  group_by(main_list) %>%\n",
    "  summarise(\n",
    "    n_posts = n(),\n",
    "    n_accounts = n_distinct(surface.id),\n",
    "    posts_per_account = round(n_posts / n_accounts, 1),\n",
    "    .groups = \"drop\"\n",
    "  )\n",
    "\n",
    "cat(\"Posts and accounts by main_list:\\n\")\n",
    "print(list_summary)\n",
    "cat(\"\\n\")\n",
    "\n",
    "# Check if totals match\n",
    "total_unique_accounts <- n_distinct(data$surface.id)\n",
    "sum_accounts_by_list <- sum(list_summary$n_accounts)\n",
    "\n",
    "if (length(integrity_errors$multi_list_accounts) > 0) {\n",
    "  expected_overcounting <- nrow(integrity_errors$multi_list_accounts) * \n",
    "    (mean(integrity_errors$multi_list_accounts$n_lists) - 1)\n",
    "  cat(\"Note: Sum of accounts by list (\", sum_accounts_by_list, \n",
    "      \") > unique accounts (\", total_unique_accounts, \")\\n\", sep = \"\")\n",
    "  cat(\"This is expected due to\", nrow(integrity_errors$multi_list_accounts), \n",
    "      \"accounts appearing in multiple lists\\n\\n\")\n",
    "} else {\n",
    "  if (sum_accounts_by_list == total_unique_accounts) {\n",
    "    cat(\"✓ PASSED: Account counts are consistent (sum by list = unique total)\\n\\n\")\n",
    "  } else {\n",
    "    cat(\"⚠ Unexpected: Sum by list (\", sum_accounts_by_list, \n",
    "        \") != unique total (\", total_unique_accounts, \")\\n\\n\", sep = \"\")\n",
    "  }\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# INTEGRITY CHECK SUMMARY & DECISION\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "cat(\"=\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\")\n",
    "cat(\"MERGE INTEGRITY CHECK SUMMARY\\n\")\n",
    "cat(\"=\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n",
    "\n",
    "has_critical_errors <- length(integrity_errors) > 0\n",
    "\n",
    "if (has_critical_errors) {\n",
    "  cat(\"✗ CRITICAL ERRORS FOUND:\\n\\n\")\n",
    "  \n",
    "  if (!is.null(integrity_errors$multi_list_accounts)) {\n",
    "    cat(\"  - \", nrow(integrity_errors$multi_list_accounts), \n",
    "        \" accounts assigned to multiple lists\\n\", sep = \"\")\n",
    "  }\n",
    "  \n",
    "  if (!is.null(integrity_errors$duplicate_posts)) {\n",
    "    cat(\"  - \", n_distinct(integrity_errors$duplicate_posts$id), \n",
    "        \" duplicate post IDs\\n\", sep = \"\")\n",
    "  }\n",
    "  \n",
    "  cat(\"\\n\")\n",
    "  cat(\"=\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\")\n",
    "  cat(\"RESOLUTION OPTIONS\\n\")\n",
    "  cat(\"=\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n",
    "  \n",
    "  # ---------------------------------------------------------------------------\n",
    "  # HANDLE MULTI-LIST ACCOUNTS\n",
    "  # ---------------------------------------------------------------------------\n",
    "  \n",
    "  if (!is.null(integrity_errors$multi_list_accounts)) {\n",
    "    cat(\"For accounts assigned to multiple lists, choose a resolution:\\n\\n\")\n",
    "    cat(\"  Option 1: STOP - Fix the merge script and re-run\\n\")\n",
    "    cat(\"  Option 2: REMOVE - Remove all posts from affected accounts\\n\")\n",
    "    cat(\"  Option 3: MAJORITY - Assign account to list with most posts\\n\")\n",
    "    cat(\"  Option 4: PRIORITY - Use priority order (MPs > Prominent > Extremists)\\n\")\n",
    "    cat(\"\\n\")\n",
    "    \n",
    "    # Set default resolution here (can be changed)\n",
    "    # PRIORITY is recommended: assigns to most specific/relevant category\n",
    "    multi_list_resolution <- \"PRIORITY\"  # Options: \"STOP\", \"REMOVE\", \"MAJORITY\", \"PRIORITY\"\n",
    "    \n",
    "    cat(\"Current setting: multi_list_resolution = '\", multi_list_resolution, \"'\\n\\n\", sep = \"\")\n",
    "    \n",
    "    if (multi_list_resolution == \"STOP\") {\n",
    "      # Save diagnostic info before stopping\n",
    "      diagnostic_file <- paste0(\"merge_integrity_errors_\", \n",
    "                                format(Sys.time(), \"%Y%m%d_%H%M%S\"), \".rds\")\n",
    "      saveRDS(integrity_errors, diagnostic_file)\n",
    "      cat(\"Diagnostic information saved to:\", diagnostic_file, \"\\n\\n\")\n",
    "      \n",
    "      stop(\"STOPPING: Critical merge integrity errors detected.\\n\",\n",
    "           \"Please review the errors above and fix the merge script.\\n\",\n",
    "           \"Alternatively, change 'multi_list_resolution' to handle automatically.\")\n",
    "      \n",
    "    } else if (multi_list_resolution == \"REMOVE\") {\n",
    "      cat(\"Removing all posts from accounts with multiple list assignments...\\n\")\n",
    "      \n",
    "      accounts_to_remove <- integrity_errors$multi_list_accounts$surface.id\n",
    "      n_posts_before <- nrow(data)\n",
    "      \n",
    "      data <- data %>%\n",
    "        filter(!(surface.id %in% accounts_to_remove))\n",
    "      \n",
    "      n_posts_removed <- n_posts_before - nrow(data)\n",
    "      cat(\"✓ Removed\", n_posts_removed, \"posts from\", \n",
    "          length(accounts_to_remove), \"accounts\\n\\n\")\n",
    "      \n",
    "    } else if (multi_list_resolution == \"MAJORITY\") {\n",
    "      cat(\"Assigning accounts to the list with most posts...\\n\")\n",
    "      \n",
    "      # For each multi-list account, determine majority list\n",
    "      majority_assignments <- data %>%\n",
    "        filter(surface.id %in% integrity_errors$multi_list_accounts$surface.id) %>%\n",
    "        count(surface.id, main_list) %>%\n",
    "        group_by(surface.id) %>%\n",
    "        slice_max(n, n = 1, with_ties = FALSE) %>%\n",
    "        ungroup() %>%\n",
    "        dplyr::select(surface.id, assigned_list = main_list)\n",
    "      \n",
    "      # Update main_list for affected accounts\n",
    "      data <- data %>%\n",
    "        left_join(majority_assignments, by = \"surface.id\") %>%\n",
    "        mutate(\n",
    "          main_list = if_else(!is.na(assigned_list), assigned_list, main_list)\n",
    "        ) %>%\n",
    "        dplyr::select(-assigned_list)\n",
    "      \n",
    "      cat(\"✓ Reassigned\", nrow(majority_assignments), \"accounts to majority list\\n\")\n",
    "      cat(\"  New distribution:\\n\")\n",
    "      print(data %>% \n",
    "              filter(surface.id %in% integrity_errors$multi_list_accounts$surface.id) %>%\n",
    "              count(main_list))\n",
    "      cat(\"\\n\")\n",
    "      \n",
    "    } else if (multi_list_resolution == \"PRIORITY\") {\n",
    "      cat(\"Assigning accounts using priority order...\\n\")\n",
    "      cat(\"  Priority: MPs_Reelected > MPs_New > Prominent_Politicians > Extremists\\n\")\n",
    "      cat(\"  (MPs = core sample; Prominent = established figures; Extremists = catch-all)\\n\\n\")\n",
    "      \n",
    "      # Define priority (lower number = higher priority)\n",
    "      # MPs are the core research sample\n",
    "      # Prominent_Politicians are established political figures\n",
    "      # Extremists is the broadest catch-all category\n",
    "      priority_order <- c(\"MPs_Reelected\" = 1, \"MPs_New\" = 2, \"MPs\" = 3, \n",
    "                          \"Prominent_Politicians\" = 4, \"Extremists\" = 5)\n",
    "      \n",
    "      # For each multi-list account, use highest priority list\n",
    "      priority_assignments <- data %>%\n",
    "        filter(surface.id %in% integrity_errors$multi_list_accounts$surface.id) %>%\n",
    "        distinct(surface.id, main_list) %>%\n",
    "        mutate(priority = priority_order[main_list]) %>%\n",
    "        group_by(surface.id) %>%\n",
    "        slice_min(priority, n = 1, with_ties = FALSE) %>%\n",
    "        ungroup() %>%\n",
    "        dplyr::select(surface.id, assigned_list = main_list)\n",
    "      \n",
    "      # Log each assignment\n",
    "      cat(\"Assigning accounts to highest-priority list:\\n\")\n",
    "      for (i in 1:nrow(priority_assignments)) {\n",
    "        acc_id <- priority_assignments$surface.id[i]\n",
    "        acc_info <- integrity_errors$multi_list_accounts %>%\n",
    "          filter(surface.id == acc_id)\n",
    "        assigned <- priority_assignments$assigned_list[i]\n",
    "        cat(sprintf(\"  %s: %s → %s\\n\", \n",
    "                    acc_info$surface_name, acc_info$lists, assigned))\n",
    "      }\n",
    "      cat(\"\\n\")\n",
    "      \n",
    "      # Update main_list for affected accounts\n",
    "      data <- data %>%\n",
    "        left_join(priority_assignments, by = \"surface.id\") %>%\n",
    "        mutate(\n",
    "          main_list = if_else(!is.na(assigned_list), assigned_list, main_list)\n",
    "        ) %>%\n",
    "        dplyr::select(-assigned_list)\n",
    "      \n",
    "      cat(\"✓ Reassigned\", nrow(priority_assignments), \"accounts using priority\\n\")\n",
    "      cat(\"  New distribution:\\n\")\n",
    "      print(data %>% \n",
    "              filter(surface.id %in% integrity_errors$multi_list_accounts$surface.id) %>%\n",
    "              count(main_list))\n",
    "      cat(\"\\n\")\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # ---------------------------------------------------------------------------\n",
    "  # HANDLE DUPLICATE POSTS\n",
    "  # ---------------------------------------------------------------------------\n",
    "  \n",
    "  if (!is.null(integrity_errors$duplicate_posts)) {\n",
    "    cat(\"Handling duplicate post IDs...\\n\")\n",
    "    \n",
    "    n_before <- nrow(data)\n",
    "    \n",
    "    # Keep first occurrence of each duplicate\n",
    "    data <- data %>%\n",
    "      distinct(id, .keep_all = TRUE)\n",
    "    \n",
    "    n_removed <- n_before - nrow(data)\n",
    "    cat(\"✓ Removed\", n_removed, \"duplicate posts (kept first occurrence)\\n\\n\")\n",
    "  }\n",
    "  \n",
    "} else {\n",
    "  cat(\"✓ ALL MERGE INTEGRITY CHECKS PASSED\\n\\n\")\n",
    "}\n",
    "\n",
    "# Warnings summary\n",
    "if (length(integrity_warnings) > 0) {\n",
    "  cat(\"Warnings (non-critical):\\n\")\n",
    "  for (warning_name in names(integrity_warnings)) {\n",
    "    cat(\"  - \", warning_name, \": \", nrow(integrity_warnings[[warning_name]]), \" cases\\n\", sep = \"\")\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# FINAL VERIFICATION: Re-check that each surface.id now has exactly ONE list\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "cat(\"FINAL VERIFICATION: Re-checking list assignments...\\n\")\n",
    "\n",
    "final_check <- data %>%\n",
    "  group_by(surface.id) %>%\n",
    "  summarise(n_lists = n_distinct(main_list), .groups = \"drop\") %>%\n",
    "  filter(n_lists > 1)\n",
    "\n",
    "if (nrow(final_check) > 0) {\n",
    "  stop(\"FATAL: After resolution, \", nrow(final_check), \n",
    "       \" accounts still have multiple list assignments!\")\n",
    "} else {\n",
    "  cat(\"✓ VERIFIED: All\", n_distinct(data$surface.id), \n",
    "      \"accounts now have exactly ONE list assignment\\n\\n\")\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: CRITICAL FIELD VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "cat(\"STEP 3: Validating critical fields...\\n\")\n",
    "cat(\"-\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n",
    "\n",
    "# Check for required fields\n",
    "required_fields <- c(\"id\", \"surface.id\", \"main_list\", \"date\", \"creation_time\",\n",
    "                     \"statistics.views\", \"statistics.reaction_count\", \n",
    "                     \"statistics.share_count\", \"statistics.comment_count\")\n",
    "\n",
    "missing_fields <- setdiff(required_fields, names(data))\n",
    "if (length(missing_fields) > 0) {\n",
    "  stop(\"Missing required fields: \", paste(missing_fields, collapse = \", \"))\n",
    "}\n",
    "\n",
    "cat(\"✓ All required fields present\\n\\n\")\n",
    "\n",
    "# Validate surface.id\n",
    "na_surface_id <- sum(is.na(data$surface.id))\n",
    "if (na_surface_id > 0) {\n",
    "  cat(\"⚠ WARNING:\", na_surface_id, \"posts with NA surface.id\\n\")\n",
    "  cat(\"These will be removed.\\n\\n\")\n",
    "} else {\n",
    "  cat(\"✓ No NA values in surface.id\\n\\n\")\n",
    "}\n",
    "\n",
    "# Validate date\n",
    "na_date <- sum(is.na(data$date))\n",
    "if (na_date > 0) {\n",
    "  cat(\"⚠ WARNING:\", na_date, \"posts with NA date\\n\")\n",
    "  cat(\"These will be removed.\\n\\n\")\n",
    "} else {\n",
    "  cat(\"✓ No NA values in date\\n\\n\")\n",
    "}\n",
    "\n",
    "# Validate main_list\n",
    "cat(\"Main list distribution (after integrity checks):\\n\")\n",
    "main_list_counts <- data %>% count(main_list)\n",
    "print(main_list_counts)\n",
    "cat(\"\\n\")\n",
    "\n",
    "# Valid main_lists - flexible to handle both v3.1 and v3.2 outputs\n",
    "valid_main_lists_pattern <- \"^(MPs.*|Prominent_Politicians|Extremists)$\"\n",
    "\n",
    "invalid_lists <- data %>%\n",
    "  filter(!grepl(valid_main_lists_pattern, main_list)) %>%\n",
    "  distinct(main_list) %>%\n",
    "  pull(main_list)\n",
    "\n",
    "if (length(invalid_lists) > 0) {\n",
    "  cat(\"⚠ WARNING: Invalid main_list values found:\\n\")\n",
    "  print(invalid_lists)\n",
    "  cat(\"\\nExpected pattern: MPs.*, Prominent_Politicians, or Extremists\\n\")\n",
    "  cat(\"These will be removed.\\n\\n\")\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: REMOVE INVALID RECORDS AND APPLY TIME-FRAME FILTER\n",
    "# ============================================================================\n",
    "\n",
    "cat(\"STEP 4: Removing invalid records and applying time-frame filter...\\n\")\n",
    "cat(\"-\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n",
    "\n",
    "# Report study time-frame\n",
    "cat(\"Study time-frame:\\n\")\n",
    "cat(\"  Start date:\", format(study_start_date, \"%Y-%m-%d\"), \"\\n\")\n",
    "cat(\"  End date:  \", format(study_end_date, \"%Y-%m-%d\"), \"\\n\\n\")\n",
    "\n",
    "n_before <- nrow(data)\n",
    "\n",
    "# Check posts outside time-frame BEFORE filtering\n",
    "posts_before_start <- sum(data$date < study_start_date, na.rm = TRUE)\n",
    "posts_after_end <- sum(data$date > study_end_date, na.rm = TRUE)\n",
    "posts_outside_timeframe <- posts_before_start + posts_after_end\n",
    "\n",
    "if (posts_outside_timeframe > 0) {\n",
    "  cat(\"Posts outside study time-frame:\\n\")\n",
    "  cat(\"  Before\", format(study_start_date, \"%Y-%m-%d\"), \":\", posts_before_start, \"\\n\")\n",
    "  cat(\"  After\", format(study_end_date, \"%Y-%m-%d\"), \" :\", posts_after_end, \"\\n\")\n",
    "  cat(\"  Total to exclude:\", posts_outside_timeframe, \"\\n\\n\")\n",
    "  \n",
    "  # Show date range of excluded posts\n",
    "  if (posts_before_start > 0) {\n",
    "    early_dates <- data %>% \n",
    "      filter(date < study_start_date) %>% \n",
    "      summarise(min = min(date), max = max(date))\n",
    "    cat(\"  Early posts range:\", format(early_dates$min, \"%Y-%m-%d\"), \"to\", \n",
    "        format(early_dates$max, \"%Y-%m-%d\"), \"\\n\")\n",
    "  }\n",
    "  if (posts_after_end > 0) {\n",
    "    late_dates <- data %>% \n",
    "      filter(date > study_end_date) %>% \n",
    "      summarise(min = min(date), max = max(date))\n",
    "    cat(\"  Late posts range:\", format(late_dates$min, \"%Y-%m-%d\"), \"to\", \n",
    "        format(late_dates$max, \"%Y-%m-%d\"), \"\\n\")\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# Apply all filters including time-frame\n",
    "data <- data %>%\n",
    "  filter(\n",
    "    !is.na(surface.id),\n",
    "    !is.na(date),\n",
    "    grepl(valid_main_lists_pattern, main_list),\n",
    "    date >= study_start_date,\n",
    "    date <= study_end_date\n",
    "  )\n",
    "\n",
    "n_removed <- n_before - nrow(data)\n",
    "n_removed_other <- n_removed - posts_outside_timeframe\n",
    "\n",
    "cat(\"Records removed:\\n\")\n",
    "cat(\"  Invalid (NA surface.id, NA date, invalid list):\", n_removed_other, \"\\n\")\n",
    "cat(\"  Outside study time-frame:\", posts_outside_timeframe, \"\\n\")\n",
    "cat(\"  Total removed:\", n_removed, \"\\n\")\n",
    "cat(\"Remaining posts:\", nrow(data), \"\\n\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: TEMPORAL COVERAGE VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "cat(\"STEP 5: Validating temporal coverage...\\n\")\n",
    "cat(\"-\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n",
    "\n",
    "# Overall date range\n",
    "date_range <- data %>%\n",
    "  summarise(\n",
    "    min_date = min(date, na.rm = TRUE),\n",
    "    max_date = max(date, na.rm = TRUE),\n",
    "    span_days = as.numeric(difftime(max(date), min(date), units = \"days\"))\n",
    "  )\n",
    "\n",
    "cat(\"Overall date range:\\n\")\n",
    "cat(\"  First post:\", format(date_range$min_date, \"%Y-%m-%d\"), \"\\n\")\n",
    "cat(\"  Last post:\", format(date_range$max_date, \"%Y-%m-%d\"), \"\\n\")\n",
    "cat(\"  Total span:\", date_range$span_days, \"days\\n\\n\")\n",
    "\n",
    "# Date range by main list\n",
    "cat(\"Date range by main list:\\n\")\n",
    "date_by_list <- data %>%\n",
    "  group_by(main_list) %>%\n",
    "  summarise(\n",
    "    first_post = min(date),\n",
    "    last_post = max(date),\n",
    "    span_days = as.numeric(difftime(last_post, first_post, units = \"days\")),\n",
    "    n_posts = n(),\n",
    "    .groups = \"drop\"\n",
    "  )\n",
    "print(date_by_list)\n",
    "cat(\"\\n\")\n",
    "\n",
    "# Check for gaps\n",
    "cat(\"Checking for temporal gaps...\\n\")\n",
    "daily_posts <- data %>%\n",
    "  count(date) %>%\n",
    "  arrange(date)\n",
    "\n",
    "# Find gaps > 30 days\n",
    "gaps <- daily_posts %>%\n",
    "  mutate(\n",
    "    next_date = lead(date),\n",
    "    gap_days = as.numeric(difftime(next_date, date, units = \"days\"))\n",
    "  ) %>%\n",
    "  filter(gap_days > 30) %>%\n",
    "  dplyr::select(date, next_date, gap_days)\n",
    "\n",
    "if (nrow(gaps) > 0) {\n",
    "  cat(\"⚠ WARNING: Found\", nrow(gaps), \"gaps > 30 days:\\n\")\n",
    "  print(gaps)\n",
    "  cat(\"\\n\")\n",
    "} else {\n",
    "  cat(\"✓ No major temporal gaps detected\\n\\n\")\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: KEY META POLICY DATES\n",
    "# ============================================================================\n",
    "\n",
    "cat(\"STEP 6: Defining Meta policy periods...\\n\")\n",
    "cat(\"-\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n",
    "\n",
    "# Define key policy dates\n",
    "policy_dates <- tibble(\n",
    "  event = c(\n",
    "    \"Pre-Policy\",\n",
    "    \"Initial Announcement\",\n",
    "    \"US Tests Begin\",\n",
    "    \"Global Expansion 1\",\n",
    "    \"Reduced Engagement Weight Test\",\n",
    "    \"Global Implementation\",\n",
    "    \"Refinements (Survey Signals)\",\n",
    "    \"User Control Setting\",\n",
    "    \"Policy Reversal\",\n",
    "    \"Updated Approach\"\n",
    "  ),\n",
    "  date = as.Date(c(\n",
    "    \"2020-01-01\",\n",
    "    \"2021-02-10\",\n",
    "    config$policy_timeline$initial_tests,\n",
    "    config$policy_timeline$european_expansion,\n",
    "    \"2022-05-24\",\n",
    "    config$policy_timeline$global_implementation,\n",
    "    config$policy_timeline$engagement_deemphasis,\n",
    "    config$policy_timeline$user_controls,\n",
    "    config$policy_timeline$reversal_announcement,\n",
    "    \"2025-05-28\"\n",
    "  ))\n",
    ")\n",
    "\n",
    "cat(\"Key policy dates:\\n\")\n",
    "print(policy_dates)\n",
    "cat(\"\\n\")\n",
    "\n",
    "# Check data coverage for key periods\n",
    "cat(\"Data coverage around key date (2022-07-19 global implementation):\\n\")\n",
    "key_date <- config$policy_timeline$global_implementation\n",
    "\n",
    "coverage_check <- data %>%\n",
    "  mutate(\n",
    "    period = case_when(\n",
    "      date < key_date - 180 ~ \"6mo before\",\n",
    "      date >= key_date - 180 & date < key_date - 90 ~ \"3-6mo before\",\n",
    "      date >= key_date - 90 & date < key_date - 30 ~ \"1-3mo before\",\n",
    "      date >= key_date - 30 & date < key_date ~ \"1mo before\",\n",
    "      date >= key_date & date < key_date + 30 ~ \"1mo after\",\n",
    "      date >= key_date + 30 & date < key_date + 90 ~ \"1-3mo after\",\n",
    "      date >= key_date + 90 & date < key_date + 180 ~ \"3-6mo after\",\n",
    "      date >= key_date + 180 ~ \"6mo+ after\"\n",
    "    )\n",
    "  ) %>%\n",
    "  count(period, main_list) %>%\n",
    "  pivot_wider(names_from = main_list, values_from = n, values_fill = 0)\n",
    "\n",
    "print(coverage_check)\n",
    "cat(\"\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: ENGAGEMENT METRICS VALIDATION AND HANDLING\n",
    "# ============================================================================\n",
    "\n",
    "cat(\"STEP 7: Validating and handling engagement metrics...\\n\")\n",
    "cat(\"-\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n",
    "\n",
    "# Set seed for reproducibility of random imputation\n",
    "set.seed(42)\n",
    "cat(\"Random seed set to 42 for reproducibility\\n\\n\")\n",
    "\n",
    "# Check for NA values in key metrics\n",
    "engagement_fields <- c(\"statistics.views\", \"statistics.reaction_count\",\n",
    "                       \"statistics.share_count\", \"statistics.comment_count\")\n",
    "\n",
    "cat(\"NA values in engagement metrics (BEFORE any handling):\\n\")\n",
    "for (field in engagement_fields) {\n",
    "  na_count <- sum(is.na(data[[field]]))\n",
    "  pct <- round(100 * na_count / nrow(data), 2)\n",
    "  cat(sprintf(\"  %-30s: %8d (%5.2f%%)\\n\", field, na_count, pct))\n",
    "}\n",
    "cat(\"\\n\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# STEP 7a: Handle NA Views - GROUP-SPECIFIC RATIO-WEIGHTED IMPUTATION\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "cat(\"--- Handling NA Views (GROUP-SPECIFIC METHOD) ---\\n\\n\")\n",
    "\n",
    "cat(\"IMPUTATION METHOD: Group-Specific Ratio-Weighted\\n\")\n",
    "cat(\"  Each group's parameters derived from its own near-threshold distribution\\n\")\n",
    "cat(\"  Power law fit to bin counts (101-\", IMPUTATION_MAX_VIEW, \" views)\\n\", sep = \"\")\n",
    "cat(\"  Addresses heterogeneity across political groups\\n\\n\")\n",
    "\n",
    "# Document NA views by group BEFORE imputation\n",
    "cat(\"NA views by group (BEFORE imputation):\\n\")\n",
    "na_views_by_group <- data %>%\n",
    "  group_by(main_list) %>%\n",
    "  summarise(\n",
    "    total_posts = n(),\n",
    "    na_views = sum(is.na(statistics.views)),\n",
    "    pct_na = round(100 * na_views / total_posts, 2),\n",
    "    .groups = \"drop\"\n",
    "  ) %>%\n",
    "  arrange(desc(pct_na))\n",
    "\n",
    "print(na_views_by_group)\n",
    "cat(\"\\n\")\n",
    "\n",
    "# Check for pre-2017 posts\n",
    "pre_2017_posts <- sum(data$date < as.Date(\"2017-01-01\"), na.rm = TRUE)\n",
    "if (pre_2017_posts > 0) {\n",
    "  cat(\"⚠ NOTE:\", pre_2017_posts, \"posts are from before 2017-01-01\\n\")\n",
    "  cat(\"  MCL documentation states view counts are not available for posts before this date.\\n\\n\")\n",
    "}\n",
    "\n",
    "# Determine reason for NA views\n",
    "data <- data %>%\n",
    "  mutate(\n",
    "    views_na_reason = case_when(\n",
    "      !is.na(statistics.views) ~ \"not_na\",\n",
    "      date < as.Date(\"2017-01-01\") ~ \"pre_2017\",\n",
    "      TRUE ~ \"under_threshold\"\n",
    "    ),\n",
    "    views_imputed = views_na_reason == \"under_threshold\",\n",
    "    views_pre_2017 = views_na_reason == \"pre_2017\"\n",
    "  )\n",
    "\n",
    "# Report NA reasons\n",
    "cat(\"NA views breakdown by reason:\\n\")\n",
    "na_reason_summary <- data %>%\n",
    "  count(views_na_reason) %>%\n",
    "  mutate(pct = round(100 * n / sum(n), 2))\n",
    "print(na_reason_summary)\n",
    "cat(\"\\n\")\n",
    "\n",
    "# Count posts to impute\n",
    "n_to_impute <- sum(data$views_imputed)\n",
    "n_pre_2017 <- sum(data$views_pre_2017)\n",
    "\n",
    "if (n_to_impute > 0) {\n",
    "  cat(\"ℹ IMPUTING\", n_to_impute, \"posts with NA views (≤100 views threshold)\\n\")\n",
    "  cat(\"  Rationale: MCL confirmed NA means ≤100 views (censoring threshold)\\n\")\n",
    "  cat(\"  Method: GROUP-SPECIFIC ratio-weighted based on each group's distribution\\n\\n\")\n",
    "  \n",
    "  # Set master seed for reproducibility\n",
    "  set.seed(42)\n",
    "  cat(\"  Master random seed: 42\\n\\n\")\n",
    "  \n",
    "  # Derive parameters for each group\n",
    "  groups <- unique(data$main_list)\n",
    "  group_imputation_params <- list()\n",
    "  \n",
    "  cat(\"Deriving imputation parameters by group:\\n\")\n",
    "  cat(\"-\" %>% rep(60) %>% paste0(collapse = \"\"), \"\\n\\n\")\n",
    "  \n",
    "  for (g in groups) {\n",
    "    # Get observed views for this group\n",
    "    group_views <- data %>%\n",
    "      filter(main_list == g, !is.na(statistics.views)) %>%\n",
    "      pull(statistics.views)\n",
    "    \n",
    "    # Derive ratio from near-threshold distribution\n",
    "    params <- derive_imputation_ratio(\n",
    "      views = group_views,\n",
    "      bin_width = IMPUTATION_BIN_WIDTH,\n",
    "      max_view = IMPUTATION_MAX_VIEW,\n",
    "      min_bins = IMPUTATION_MIN_BINS,\n",
    "      min_posts = IMPUTATION_MIN_POSTS,\n",
    "      fallback_ratio = POOLED_FALLBACK_RATIO\n",
    "    )\n",
    "    \n",
    "    # Compute expected imputation statistics\n",
    "    imp_stats <- compute_imputation_stats(params$ratio)\n",
    "    \n",
    "    # Store parameters\n",
    "    group_imputation_params[[g]] <- list(\n",
    "      group = g,\n",
    "      n_censored = sum(data$main_list == g & data$views_imputed),\n",
    "      ratio = params$ratio,\n",
    "      alpha = params$alpha,\n",
    "      r_squared = params$r_squared,\n",
    "      n_near_threshold = params$n_near_threshold,\n",
    "      reliable = params$reliable,\n",
    "      expected_mean = imp_stats$mean,\n",
    "      expected_median = imp_stats$median,\n",
    "      expected_pct_under_50 = imp_stats$pct_under_50\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    cat(sprintf(\"%s:\\n\", g))\n",
    "    cat(sprintf(\"  Posts to impute: %d\\n\", group_imputation_params[[g]]$n_censored))\n",
    "    cat(sprintf(\"  Near threshold (101-%d): %d posts\\n\", IMPUTATION_MAX_VIEW, params$n_near_threshold))\n",
    "    \n",
    "    if (params$reliable) {\n",
    "      cat(sprintf(\"  Power law exponent (α): %.3f\\n\", params$alpha))\n",
    "      cat(sprintf(\"  R-squared: %.3f\\n\", params$r_squared))\n",
    "      cat(sprintf(\"  Derived ratio (1-50 vs 50-100): %.2f\\n\", params$ratio))\n",
    "      cat(sprintf(\"  → Expected imputed mean: %.1f, median: %d\\n\", imp_stats$mean, imp_stats$median))\n",
    "    } else {\n",
    "      cat(sprintf(\"  ⚠ Using fallback ratio: %.2f (uniform)\\n\", params$ratio))\n",
    "    }\n",
    "    cat(\"\\n\")\n",
    "  }\n",
    "  \n",
    "  # Apply imputation by group\n",
    "  cat(\"Applying imputation by group:\\n\")\n",
    "  cat(\"-\" %>% rep(40) %>% paste0(collapse = \"\"), \"\\n\")\n",
    "  \n",
    "  imputation_log <- list()\n",
    "  \n",
    "  for (g in groups) {\n",
    "    params <- group_imputation_params[[g]]\n",
    "    \n",
    "    # Get indices of censored posts in this group\n",
    "    idx <- which(data$main_list == g & data$views_imputed)\n",
    "    n_group_impute <- length(idx)\n",
    "    \n",
    "    if (n_group_impute > 0) {\n",
    "      # Generate imputed values (master seed already set)\n",
    "      imputed_values <- generate_imputed_values(n_group_impute, params$ratio, seed = NULL)\n",
    "      \n",
    "      # Apply to data\n",
    "      data$statistics.views[idx] <- imputed_values\n",
    "      \n",
    "      # Log results\n",
    "      imputation_log[[g]] <- list(\n",
    "        n_imputed = n_group_impute,\n",
    "        ratio_used = params$ratio,\n",
    "        actual_mean = round(mean(imputed_values), 1),\n",
    "        actual_median = median(imputed_values),\n",
    "        actual_pct_under_50 = round(100 * mean(imputed_values <= 50), 1)\n",
    "      )\n",
    "      \n",
    "      cat(sprintf(\"  %s: %d posts (ratio=%.2f, mean=%.1f, median=%d)\\n\",\n",
    "                  g, n_group_impute, params$ratio, mean(imputed_values), median(imputed_values)))\n",
    "    } else {\n",
    "      imputation_log[[g]] <- list(n_imputed = 0)\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  cat(\"\\n✓ Imputed\", n_to_impute, \"view counts using group-specific method\\n\\n\")\n",
    "  \n",
    "  # Summary table\n",
    "  cat(\"Group-specific imputation summary:\\n\")\n",
    "  imp_summary <- tibble(\n",
    "    Group = sapply(group_imputation_params, function(x) x$group),\n",
    "    N_Imputed = sapply(group_imputation_params, function(x) x$n_censored),\n",
    "    Alpha = sapply(group_imputation_params, function(x) round(x$alpha, 3)),\n",
    "    Ratio = sapply(group_imputation_params, function(x) round(x$ratio, 2)),\n",
    "    Exp_Mean = sapply(group_imputation_params, function(x) x$expected_mean)\n",
    "  )\n",
    "  print(imp_summary)\n",
    "  cat(\"\\n\")\n",
    "  \n",
    "} else {\n",
    "  data$views_imputed <- FALSE\n",
    "  group_imputation_params <- list()\n",
    "  imputation_log <- list()\n",
    "  cat(\"✓ No views to impute\\n\\n\")\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# STEP 7b: Handle NA Engagement Metrics - REMOVE POSTS\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "cat(\"--- Handling NA Engagement Metrics (REMOVAL) ---\\n\\n\")\n",
    "\n",
    "n_before_engagement_removal <- nrow(data)\n",
    "\n",
    "# Count NAs before removal\n",
    "na_reactions <- sum(is.na(data$statistics.reaction_count))\n",
    "na_shares <- sum(is.na(data$statistics.share_count))\n",
    "na_comments <- sum(is.na(data$statistics.comment_count))\n",
    "\n",
    "cat(\"Posts with NA engagement metrics (BEFORE removal):\\n\")\n",
    "cat(\"  NA reactions:\", na_reactions, sprintf(\"(%.2f%%)\\n\", 100*na_reactions/nrow(data)))\n",
    "cat(\"  NA shares:\", na_shares, sprintf(\"(%.2f%%)\\n\", 100*na_shares/nrow(data)))\n",
    "cat(\"  NA comments:\", na_comments, sprintf(\"(%.2f%%)\\n\", 100*na_comments/nrow(data)))\n",
    "cat(\"\\n\")\n",
    "\n",
    "# Document by group before removal\n",
    "na_engagement_by_group <- data %>%\n",
    "  group_by(main_list) %>%\n",
    "  summarise(\n",
    "    total_posts = n(),\n",
    "    na_reactions = sum(is.na(statistics.reaction_count)),\n",
    "    na_shares = sum(is.na(statistics.share_count)),\n",
    "    na_comments = sum(is.na(statistics.comment_count)),\n",
    "    posts_any_na = sum(is.na(statistics.reaction_count) | \n",
    "                       is.na(statistics.share_count) | \n",
    "                       is.na(statistics.comment_count)),\n",
    "    pct_any_na = round(100 * posts_any_na / total_posts, 2),\n",
    "    .groups = \"drop\"\n",
    "  ) %>%\n",
    "  arrange(desc(pct_any_na))\n",
    "\n",
    "cat(\"NA engagement by group (BEFORE removal):\\n\")\n",
    "print(na_engagement_by_group)\n",
    "cat(\"\\n\")\n",
    "\n",
    "# REMOVE posts with NA engagement\n",
    "cat(\"⚠ REMOVING posts with NA engagement metrics...\\n\")\n",
    "\n",
    "data <- data %>%\n",
    "  filter(\n",
    "    !is.na(statistics.reaction_count),\n",
    "    !is.na(statistics.share_count),\n",
    "    !is.na(statistics.comment_count)\n",
    "  )\n",
    "\n",
    "n_engagement_removed <- n_before_engagement_removal - nrow(data)\n",
    "\n",
    "cat(\"✓ Removed\", n_engagement_removed, \"posts with NA engagement metrics\\n\")\n",
    "cat(\"  Remaining posts:\", nrow(data), \"\\n\\n\")\n",
    "\n",
    "# Update n_removed\n",
    "n_removed <- n_removed + n_engagement_removed\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# STEP 7c: Final Validation Checks\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "cat(\"--- Final Validation Checks ---\\n\\n\")\n",
    "\n",
    "# Check for negative values\n",
    "negative_checks <- data %>%\n",
    "  summarise(\n",
    "    negative_views = sum(statistics.views < 0, na.rm = TRUE),\n",
    "    negative_reactions = sum(statistics.reaction_count < 0, na.rm = TRUE),\n",
    "    negative_shares = sum(statistics.share_count < 0, na.rm = TRUE),\n",
    "    negative_comments = sum(statistics.comment_count < 0, na.rm = TRUE)\n",
    "  )\n",
    "\n",
    "if (sum(negative_checks) > 0) {\n",
    "  cat(\"⚠ WARNING: Found negative values - setting to 0\\n\")\n",
    "  data <- data %>%\n",
    "    mutate(\n",
    "      statistics.views = pmax(statistics.views, 0, na.rm = TRUE),\n",
    "      statistics.reaction_count = pmax(statistics.reaction_count, 0),\n",
    "      statistics.share_count = pmax(statistics.share_count, 0),\n",
    "      statistics.comment_count = pmax(statistics.comment_count, 0)\n",
    "    )\n",
    "} else {\n",
    "  cat(\"✓ No negative values found\\n\")\n",
    "}\n",
    "\n",
    "# Check NA content_type\n",
    "na_content_type <- sum(is.na(data$content_type))\n",
    "if (na_content_type > 0) {\n",
    "  # MCL message: \"This content isn't available right now. Content Library does not support this type of attachment.\"\n",
    "  data <- data %>%\n",
    "    mutate(content_type = replace_na(content_type, \"mcl_unsupported_attachment\"))\n",
    "  cat(\"✓ Replaced\", na_content_type, \"NA content_type with 'mcl_unsupported_attachment'\\n\")\n",
    "  cat(\"  (Original MCL message: 'Content Library does not support this type of attachment')\\n\")\n",
    "} else {\n",
    "  cat(\"✓ No NA values in content_type\\n\")\n",
    "}\n",
    "cat(\"\\n\")\n",
    "\n",
    "# Final NA check\n",
    "cat(\"NA values in engagement metrics (AFTER handling):\\n\")\n",
    "for (field in engagement_fields) {\n",
    "  na_count <- sum(is.na(data[[field]]))\n",
    "  cat(sprintf(\"  %-30s: %d\\n\", field, na_count))\n",
    "}\n",
    "cat(\"\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: CREATE ANALYSIS VARIABLES\n",
    "# ============================================================================\n",
    "\n",
    "cat(\"STEP 8: Creating analysis-ready variables...\\n\")\n",
    "cat(\"-\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n",
    "\n",
    "data <- data %>%\n",
    "  mutate(\n",
    "    # Policy period\n",
    "    policy_period = case_when(\n",
    "      date < config$policy_timeline$global_implementation ~ \"Pre-Policy\",\n",
    "      date >= config$policy_timeline$global_implementation ~ \"Post-Policy\"\n",
    "    ),\n",
    "    \n",
    "    days_since_policy = as.numeric(difftime(date, config$policy_timeline$global_implementation, units = \"days\")),\n",
    "    \n",
    "    policy_phase = case_when(\n",
    "      date < as.Date(\"2021-02-10\") ~ \"Baseline\",\n",
    "      date >= as.Date(\"2021-02-10\") & date < config$policy_timeline$european_expansion ~ \"Announcement\",\n",
    "      date >= config$policy_timeline$european_expansion & date < config$policy_timeline$global_implementation ~ \"Testing\",\n",
    "      date >= config$policy_timeline$global_implementation & date < config$policy_timeline$engagement_deemphasis ~ \"Implementation\",\n",
    "      date >= config$policy_timeline$engagement_deemphasis & date < config$policy_timeline$reversal_announcement ~ \"Refinement\",\n",
    "      date >= config$policy_timeline$reversal_announcement ~ \"Reversal\"\n",
    "    ),\n",
    "    \n",
    "    year_month = floor_date(date, \"month\"),\n",
    "    week = floor_date(date, \"week\"),\n",
    "    quarter = quarter(date, with_year = TRUE),\n",
    "    \n",
    "    total_engagement = statistics.reaction_count + statistics.share_count + statistics.comment_count,\n",
    "    engagement_rate = if_else(statistics.views > 0, total_engagement / statistics.views, 0),\n",
    "    \n",
    "    log_views = log1p(statistics.views),\n",
    "    log_reactions = log1p(statistics.reaction_count),\n",
    "    log_shares = log1p(statistics.share_count),\n",
    "    log_comments = log1p(statistics.comment_count),\n",
    "    log_total_engagement = log1p(total_engagement)\n",
    "  )\n",
    "\n",
    "cat(\"✓ Created all analysis variables\\n\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 9: ACCOUNT-LEVEL AGGREGATIONS\n",
    "# ============================================================================\n",
    "\n",
    "cat(\"STEP 9: Creating account-level dataset...\\n\")\n",
    "cat(\"-\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n",
    "\n",
    "accounts <- data %>%\n",
    "  group_by(surface.id, surface.name, surface.username, main_list) %>%\n",
    "  summarise(\n",
    "    n_posts = n(),\n",
    "    n_posts_pre = sum(policy_period == \"Pre-Policy\"),\n",
    "    n_posts_post = sum(policy_period == \"Post-Policy\"),\n",
    "    n_views_imputed = sum(views_imputed),\n",
    "    pct_views_imputed = round(100 * n_views_imputed / n_posts, 2),\n",
    "    first_post = min(date),\n",
    "    last_post = max(date),\n",
    "    days_active = as.numeric(difftime(last_post, first_post, units = \"days\")),\n",
    "    sub_list = first(sub_list),\n",
    "    total_views = sum(statistics.views, na.rm = TRUE),\n",
    "    total_reactions = sum(statistics.reaction_count, na.rm = TRUE),\n",
    "    total_shares = sum(statistics.share_count, na.rm = TRUE),\n",
    "    total_comments = sum(statistics.comment_count, na.rm = TRUE),\n",
    "    total_engagement = sum(total_engagement, na.rm = TRUE),\n",
    "    avg_views = mean(statistics.views, na.rm = TRUE),\n",
    "    avg_reactions = mean(statistics.reaction_count, na.rm = TRUE),\n",
    "    avg_shares = mean(statistics.share_count, na.rm = TRUE),\n",
    "    avg_comments = mean(statistics.comment_count, na.rm = TRUE),\n",
    "    avg_engagement_rate = mean(engagement_rate, na.rm = TRUE),\n",
    "    median_views = median(statistics.views, na.rm = TRUE),\n",
    "    median_reactions = median(statistics.reaction_count, na.rm = TRUE),\n",
    "    avg_views_pre = mean(statistics.views[policy_period == \"Pre-Policy\"], na.rm = TRUE),\n",
    "    avg_views_post = mean(statistics.views[policy_period == \"Post-Policy\"], na.rm = TRUE),\n",
    "    .groups = \"drop\"\n",
    "  ) %>%\n",
    "  mutate(\n",
    "    reach_change = avg_views_post - avg_views_pre,\n",
    "    reach_change_pct = if_else(avg_views_pre > 0,\n",
    "                               100 * (avg_views_post - avg_views_pre) / avg_views_pre,\n",
    "                               NA_real_),\n",
    "    active_pre = n_posts_pre >= 10,\n",
    "    active_post = n_posts_post >= 10,\n",
    "    active_both = active_pre & active_post\n",
    "  )\n",
    "\n",
    "cat(\"✓ Created account-level summary\\n\")\n",
    "cat(\"  Unique accounts:\", nrow(accounts), \"\\n\\n\")\n",
    "\n",
    "# FINAL CHECK: Verify no account appears in multiple rows\n",
    "account_check <- accounts %>%\n",
    "  group_by(surface.id) %>%\n",
    "  filter(n() > 1) %>%\n",
    "  ungroup()\n",
    "\n",
    "if (nrow(account_check) > 0) {\n",
    "  stop(\"FATAL: Account-level data has duplicate surface.ids!\")\n",
    "} else {\n",
    "  cat(\"✓ VERIFIED: Each account appears exactly once in accounts dataset\\n\\n\")\n",
    "}\n",
    "\n",
    "cat(\"Accounts by main list:\\n\")\n",
    "print(accounts %>% count(main_list))\n",
    "cat(\"\\n\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# CREATE COMPREHENSIVE SURFACE INFO DATASET\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "cat(\"Creating comprehensive surface info dataset...\\n\\n\")\n",
    "\n",
    "# Extract all unique surface-level fields from the post data\n",
    "# These are fields that describe the account/page itself, not individual posts\n",
    "\n",
    "# First, identify all surface.* columns in the data\n",
    "surface_columns <- names(data)[grepl(\"^surface\\\\.\", names(data))]\n",
    "cat(\"  Found\", length(surface_columns), \"surface-level columns:\\n\")\n",
    "cat(\"   \", paste(surface_columns, collapse = \", \"), \"\\n\\n\")\n",
    "\n",
    "# Create surface info by taking the most recent/complete info for each surface.id\n",
    "surface_info <- data %>%\n",
    "  # Group by surface.id to get one row per account\n",
    "  group_by(surface.id) %>%\n",
    "  # For each surface field, take the most recent non-NA value\n",
    "  summarise(\n",
    "    # Core identifiers\n",
    "    surface.name = last(na.omit(surface.name)),\n",
    "    surface.username = last(na.omit(surface.username)),\n",
    "    \n",
    "    # List assignments (should be unique after integrity checks)\n",
    "    main_list = first(main_list),\n",
    "    sub_list = first(sub_list),\n",
    "    \n",
    "    # Additional surface fields if they exist (dynamic)\n",
    "    across(\n",
    "      any_of(setdiff(surface_columns, c(\"surface.id\", \"surface.name\", \"surface.username\"))),\n",
    "      ~last(na.omit(.))\n",
    "    ),\n",
    "    \n",
    "    # Computed metadata from posts\n",
    "    n_posts_total = n(),\n",
    "    n_posts_pre_policy = sum(policy_period == \"Pre-Policy\"),\n",
    "    n_posts_post_policy = sum(policy_period == \"Post-Policy\"),\n",
    "    \n",
    "    # Temporal coverage\n",
    "    first_post_date = min(date),\n",
    "    last_post_date = max(date),\n",
    "    days_active = as.numeric(difftime(max(date), min(date), units = \"days\")),\n",
    "    \n",
    "    # Activity spans policy change?\n",
    "    spans_policy_change = (min(date) < config$policy_timeline$global_implementation) & \n",
    "                          (max(date) >= config$policy_timeline$global_implementation),\n",
    "    \n",
    "    # Engagement summary (lifetime)\n",
    "    total_views = sum(statistics.views, na.rm = TRUE),\n",
    "    total_reactions = sum(statistics.reaction_count, na.rm = TRUE),\n",
    "    total_shares = sum(statistics.share_count, na.rm = TRUE),\n",
    "    total_comments = sum(statistics.comment_count, na.rm = TRUE),\n",
    "    total_engagement = sum(total_engagement, na.rm = TRUE),\n",
    "    \n",
    "    # Average engagement per post\n",
    "    avg_views_per_post = mean(statistics.views, na.rm = TRUE),\n",
    "    avg_reactions_per_post = mean(statistics.reaction_count, na.rm = TRUE),\n",
    "    avg_shares_per_post = mean(statistics.share_count, na.rm = TRUE),\n",
    "    avg_comments_per_post = mean(statistics.comment_count, na.rm = TRUE),\n",
    "    \n",
    "    # Median engagement (more robust)\n",
    "    median_views_per_post = median(statistics.views, na.rm = TRUE),\n",
    "    median_reactions_per_post = median(statistics.reaction_count, na.rm = TRUE),\n",
    "    \n",
    "    # Engagement rate\n",
    "    avg_engagement_rate = mean(engagement_rate, na.rm = TRUE),\n",
    "    \n",
    "    # Pre vs Post policy comparison\n",
    "    avg_views_pre_policy = mean(statistics.views[policy_period == \"Pre-Policy\"], na.rm = TRUE),\n",
    "    avg_views_post_policy = mean(statistics.views[policy_period == \"Post-Policy\"], na.rm = TRUE),\n",
    "    avg_reactions_pre_policy = mean(statistics.reaction_count[policy_period == \"Pre-Policy\"], na.rm = TRUE),\n",
    "    avg_reactions_post_policy = mean(statistics.reaction_count[policy_period == \"Post-Policy\"], na.rm = TRUE),\n",
    "    \n",
    "    # Content type breakdown\n",
    "    n_content_types = n_distinct(content_type),\n",
    "    primary_content_type = names(sort(table(content_type), decreasing = TRUE))[1],\n",
    "    \n",
    "    # Data quality flags\n",
    "    n_views_imputed = sum(views_imputed),\n",
    "    pct_views_imputed = round(100 * sum(views_imputed) / n(), 2),\n",
    "    \n",
    "    .groups = \"drop\"\n",
    "  ) %>%\n",
    "  # Add computed change metrics\n",
    "mutate(\n",
    "    reach_change = avg_views_post_policy - avg_views_pre_policy,\n",
    "    reach_change_pct = if_else(\n",
    "      !is.na(avg_views_pre_policy) & avg_views_pre_policy > 0,\n",
    "      100 * (avg_views_post_policy - avg_views_pre_policy) / avg_views_pre_policy,\n",
    "      NA_real_\n",
    "    ),\n",
    "    reactions_change = avg_reactions_post_policy - avg_reactions_pre_policy,\n",
    "    reactions_change_pct = if_else(\n",
    "      !is.na(avg_reactions_pre_policy) & avg_reactions_pre_policy > 0,\n",
    "      100 * (avg_reactions_post_policy - avg_reactions_pre_policy) / avg_reactions_pre_policy,\n",
    "      NA_real_\n",
    "    )\n",
    "  ) %>%\n",
    "  # Reorder columns for clarity\n",
    "  dplyr::select(\n",
    "    # Identifiers first\n",
    "    surface.id, surface.name, surface.username,\n",
    "    # List assignments\n",
    "    main_list, sub_list,\n",
    "    # Any other surface.* columns\n",
    "    starts_with(\"surface.\"),\n",
    "    # Activity metrics\n",
    "    n_posts_total, n_posts_pre_policy, n_posts_post_policy,\n",
    "    first_post_date, last_post_date, days_active, spans_policy_change,\n",
    "    # Engagement totals\n",
    "    total_views, total_reactions, total_shares, total_comments, total_engagement,\n",
    "    # Per-post averages\n",
    "    avg_views_per_post, avg_reactions_per_post, avg_shares_per_post, avg_comments_per_post,\n",
    "    median_views_per_post, median_reactions_per_post, avg_engagement_rate,\n",
    "    # Pre/post comparison\n",
    "    avg_views_pre_policy, avg_views_post_policy, reach_change, reach_change_pct,\n",
    "    avg_reactions_pre_policy, avg_reactions_post_policy, reactions_change, reactions_change_pct,\n",
    "    # Content info\n",
    "    n_content_types, primary_content_type,\n",
    "    # Data quality\n",
    "    n_views_imputed, pct_views_imputed,\n",
    "    # Anything else\n",
    "    everything()\n",
    "  )\n",
    "\n",
    "cat(\"✓ Created surface info dataset\\n\")\n",
    "cat(\"  Total surfaces:\", nrow(surface_info), \"\\n\")\n",
    "cat(\"  Columns:\", ncol(surface_info), \"\\n\\n\")\n",
    "\n",
    "# Verify one-to-one mapping with main_list\n",
    "surface_list_check <- surface_info %>%\n",
    "  group_by(surface.id) %>%\n",
    "  filter(n() > 1)\n",
    "\n",
    "if (nrow(surface_list_check) > 0) {\n",
    "  stop(\"FATAL: Surface info has duplicate surface.ids!\")\n",
    "} else {\n",
    "  cat(\"✓ VERIFIED: Each surface appears exactly once in surface_info\\n\\n\")\n",
    "}\n",
    "\n",
    "# Summary by main_list\n",
    "cat(\"Surface info summary by main_list:\\n\")\n",
    "surface_summary <- surface_info %>%\n",
    "  group_by(main_list) %>%\n",
    "  summarise(\n",
    "    n_surfaces = n(),\n",
    "    n_spanning_policy = sum(spans_policy_change),\n",
    "    avg_posts = round(mean(n_posts_total), 1),\n",
    "    avg_total_views = round(mean(total_views), 0),\n",
    "    avg_reach_change_pct = round(mean(reach_change_pct, na.rm = TRUE), 1),\n",
    "    .groups = \"drop\"\n",
    "  )\n",
    "print(surface_summary)\n",
    "cat(\"\\n\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# CREATE SURFACE ID LIST FOR MCL API QUERIES\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "cat(\"Creating surface ID list for MCL API queries...\\n\\n\")\n",
    "\n",
    "# Create a simple list of surface.ids for use with the MCL API script\n",
    "surface_ids_for_api <- surface_info %>%\n",
    "  dplyr::select(surface.id, surface.name, main_list) %>%\n",
    "  arrange(main_list, surface.name)\n",
    "\n",
    "cat(\"  Total surface IDs for API query:\", nrow(surface_ids_for_api), \"\\n\")\n",
    "cat(\"  By main_list:\\n\")\n",
    "print(surface_ids_for_api %>% count(main_list))\n",
    "cat(\"\\n\")\n",
    "\n",
    "accounts_both_periods <- accounts %>% filter(active_both)\n",
    "\n",
    "cat(\"Accounts active in BOTH periods (≥10 posts each):\\n\")\n",
    "print(accounts_both_periods %>% count(main_list))\n",
    "cat(\"\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 10: DATA QUALITY CHECKS\n",
    "# ============================================================================\n",
    "\n",
    "cat(\"STEP 10: Final data quality checks...\\n\")\n",
    "cat(\"-\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates <- data %>%\n",
    "  group_by(id) %>%\n",
    "  filter(n() > 1) %>%\n",
    "  ungroup()\n",
    "\n",
    "if (nrow(duplicates) > 0) {\n",
    "  cat(\"⚠ WARNING: Removing\", n_distinct(duplicates$id), \"duplicate post IDs\\n\")\n",
    "  data <- data %>% distinct(id, .keep_all = TRUE)\n",
    "} else {\n",
    "  cat(\"✓ No duplicate posts\\n\")\n",
    "}\n",
    "\n",
    "# Period coverage\n",
    "cat(\"\\nPosts per main list in each period:\\n\")\n",
    "period_coverage <- data %>%\n",
    "  count(main_list, policy_period) %>%\n",
    "  pivot_wider(names_from = policy_period, values_from = n, values_fill = 0) %>%\n",
    "  mutate(Total = rowSums(across(-main_list)))\n",
    "print(period_coverage)\n",
    "cat(\"\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 11: CREATE ANALYSIS-READY DATASETS\n",
    "# ============================================================================\n",
    "\n",
    "cat(\"STEP 11: Creating analysis-ready datasets...\\n\")\n",
    "cat(\"-\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n",
    "\n",
    "cleaned_data <- data\n",
    "\n",
    "data_both_periods <- data %>%\n",
    "  filter(surface.id %in% accounts_both_periods$surface.id)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# COMPLETE WEEKS FILTERING FOR WEEKLY AGGREGATION\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "cat(\"--- Complete Weeks Filtering ---\\n\\n\")\n",
    "\n",
    "# Calculate complete week boundaries\n",
    "first_complete_week <- get_first_complete_week_start(study_start_date)\n",
    "last_complete_week_end <- get_last_complete_week_end(study_end_date)\n",
    "last_complete_week_start <- last_complete_week_end - days(6)\n",
    "\n",
    "cat(\"Study time-frame:\\n\")\n",
    "cat(\"  Start date:\", format(study_start_date, \"%Y-%m-%d\"), \n",
    "    \"(\", weekdays(study_start_date), \")\\n\")\n",
    "cat(\"  End date:  \", format(study_end_date, \"%Y-%m-%d\"), \n",
    "    \"(\", weekdays(study_end_date), \")\\n\\n\")\n",
    "\n",
    "cat(\"Complete weeks boundaries:\\n\")\n",
    "cat(\"  First complete week starts:\", format(first_complete_week, \"%Y-%m-%d\"), \n",
    "    \"(\", weekdays(first_complete_week), \")\\n\")\n",
    "cat(\"  Last complete week ends:   \", format(last_complete_week_end, \"%Y-%m-%d\"), \n",
    "    \"(\", weekdays(last_complete_week_end), \")\\n\\n\")\n",
    "\n",
    "# Calculate how many weeks are excluded\n",
    "all_weeks <- data %>%\n",
    "  distinct(week) %>%\n",
    "  arrange(week)\n",
    "\n",
    "complete_weeks <- all_weeks %>%\n",
    "  filter(week >= first_complete_week & week <= last_complete_week_start)\n",
    "\n",
    "incomplete_weeks_start <- all_weeks %>%\n",
    "  filter(week < first_complete_week)\n",
    "\n",
    "incomplete_weeks_end <- all_weeks %>%\n",
    "  filter(week > last_complete_week_start)\n",
    "\n",
    "n_total_weeks <- nrow(all_weeks)\n",
    "n_complete_weeks <- nrow(complete_weeks)\n",
    "n_incomplete_start <- nrow(incomplete_weeks_start)\n",
    "n_incomplete_end <- nrow(incomplete_weeks_end)\n",
    "\n",
    "cat(\"Week counts:\\n\")\n",
    "cat(\"  Total unique weeks in data:\", n_total_weeks, \"\\n\")\n",
    "cat(\"  Complete weeks:            \", n_complete_weeks, \"\\n\")\n",
    "cat(\"  Partial weeks at start:    \", n_incomplete_start, \"\\n\")\n",
    "cat(\"  Partial weeks at end:      \", n_incomplete_end, \"\\n\\n\")\n",
    "\n",
    "if (n_incomplete_start > 0) {\n",
    "  cat(\"Partial weeks at START (excluded from weekly_aggregation):\\n\")\n",
    "  for (i in 1:nrow(incomplete_weeks_start)) {\n",
    "    w <- incomplete_weeks_start$week[i]\n",
    "    w_end <- w + days(6)\n",
    "    posts_in_week <- sum(data$week == w)\n",
    "    cat(sprintf(\"  %s to %s (%d posts)\\n\", \n",
    "                format(w, \"%Y-%m-%d\"), format(w_end, \"%Y-%m-%d\"), posts_in_week))\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "if (n_incomplete_end > 0) {\n",
    "  cat(\"Partial weeks at END (excluded from weekly_aggregation):\\n\")\n",
    "  for (i in 1:nrow(incomplete_weeks_end)) {\n",
    "    w <- incomplete_weeks_end$week[i]\n",
    "    w_end <- w + days(6)\n",
    "    posts_in_week <- sum(data$week == w)\n",
    "    cat(sprintf(\"  %s to %s (%d posts)\\n\", \n",
    "                format(w, \"%Y-%m-%d\"), format(w_end, \"%Y-%m-%d\"), posts_in_week))\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "# Count posts excluded from weekly aggregation\n",
    "posts_in_incomplete_weeks <- data %>%\n",
    "  filter(week < first_complete_week | week > last_complete_week_start) %>%\n",
    "  nrow()\n",
    "\n",
    "cat(\"Posts in partial weeks:\", posts_in_incomplete_weeks, \n",
    "    sprintf(\"(%.2f%% of total)\\n\", 100 * posts_in_incomplete_weeks / nrow(data)))\n",
    "cat(\"These posts remain in cleaned_posts but are excluded from weekly_aggregation\\n\\n\")\n",
    "\n",
    "# Create weekly data with ONLY complete weeks\n",
    "weekly_data <- data %>%\n",
    "  # Filter to complete weeks only\n",
    "  filter(week >= first_complete_week & week <= last_complete_week_start) %>%\n",
    "  group_by(week, main_list) %>%\n",
    "  summarise(\n",
    "    n_posts = n(),\n",
    "    n_accounts = n_distinct(surface.id),\n",
    "    n_imputed = sum(views_imputed),\n",
    "    pct_imputed = round(100 * n_imputed / n_posts, 2),\n",
    "    total_views = sum(statistics.views, na.rm = TRUE),\n",
    "    avg_views = mean(statistics.views, na.rm = TRUE),\n",
    "    median_views = median(statistics.views, na.rm = TRUE),\n",
    "    avg_reactions = mean(statistics.reaction_count, na.rm = TRUE),\n",
    "    avg_shares = mean(statistics.share_count, na.rm = TRUE),\n",
    "    avg_comments = mean(statistics.comment_count, na.rm = TRUE),\n",
    "    avg_engagement_rate = mean(engagement_rate, na.rm = TRUE),\n",
    "    policy_period = first(policy_period),\n",
    "    policy_phase = first(policy_phase),\n",
    "    .groups = \"drop\"\n",
    "  ) %>%\n",
    "  # Add flag indicating this is a complete week\n",
    "  mutate(\n",
    "    week_end = week + days(6),\n",
    "    is_complete_week = TRUE\n",
    "  )\n",
    "\n",
    "cat(\"✓ Created weekly aggregation with COMPLETE WEEKS ONLY\\n\")\n",
    "cat(\"  Weekly data rows:\", nrow(weekly_data), \"\\n\")\n",
    "cat(\"  Date range:\", format(min(weekly_data$week), \"%Y-%m-%d\"), \"to\", \n",
    "    format(max(weekly_data$week_end), \"%Y-%m-%d\"), \"\\n\")\n",
    "cat(\"  Unique complete weeks:\", n_distinct(weekly_data$week), \"\\n\\n\")\n",
    "\n",
    "# Monthly aggregation (unchanged - months are always complete within study period)\n",
    "monthly_data <- data %>%\n",
    "  group_by(year_month, main_list) %>%\n",
    "  summarise(\n",
    "    n_posts = n(),\n",
    "    n_accounts = n_distinct(surface.id),\n",
    "    n_imputed = sum(views_imputed),\n",
    "    pct_imputed = round(100 * n_imputed / n_posts, 2),\n",
    "    total_views = sum(statistics.views, na.rm = TRUE),\n",
    "    avg_views = mean(statistics.views, na.rm = TRUE),\n",
    "    median_views = median(statistics.views, na.rm = TRUE),\n",
    "    avg_reactions = mean(statistics.reaction_count, na.rm = TRUE),\n",
    "    avg_shares = mean(statistics.share_count, na.rm = TRUE),\n",
    "    avg_comments = mean(statistics.comment_count, na.rm = TRUE),\n",
    "    avg_engagement_rate = mean(engagement_rate, na.rm = TRUE),\n",
    "    policy_period = first(policy_period),\n",
    "    policy_phase = first(policy_phase),\n",
    "    .groups = \"drop\"\n",
    "  )\n",
    "\n",
    "account_period_data <- data %>%\n",
    "  group_by(surface.id, surface.name, main_list, policy_period) %>%\n",
    "  summarise(\n",
    "    n_posts = n(),\n",
    "    n_views_imputed = sum(views_imputed),\n",
    "    pct_views_imputed = round(100 * n_views_imputed / n_posts, 2),\n",
    "    total_views = sum(statistics.views, na.rm = TRUE),\n",
    "    avg_views = mean(statistics.views, na.rm = TRUE),\n",
    "    median_views = median(statistics.views, na.rm = TRUE),\n",
    "    avg_reactions = mean(statistics.reaction_count, na.rm = TRUE),\n",
    "    avg_shares = mean(statistics.share_count, na.rm = TRUE),\n",
    "    avg_comments = mean(statistics.comment_count, na.rm = TRUE),\n",
    "    avg_engagement_rate = mean(engagement_rate, na.rm = TRUE),\n",
    "    .groups = \"drop\"\n",
    "  ) %>%\n",
    "  pivot_wider(\n",
    "    names_from = policy_period,\n",
    "    values_from = c(n_posts, n_views_imputed, pct_views_imputed, total_views,\n",
    "                    avg_views, median_views, avg_reactions, \n",
    "                    avg_shares, avg_comments, avg_engagement_rate),\n",
    "    names_sep = \"_\"\n",
    "  )\n",
    "\n",
    "cat(\"✓ Created 6 analysis-ready datasets:\\n\")\n",
    "cat(\"  1. cleaned_data:\", nrow(cleaned_data), \"posts\\n\")\n",
    "cat(\"  2. data_both_periods:\", nrow(data_both_periods), \"posts\\n\")\n",
    "cat(\"  3. weekly_data:\", nrow(weekly_data), \"rows (COMPLETE WEEKS ONLY)\\n\")\n",
    "cat(\"  4. monthly_data:\", nrow(monthly_data), \"rows\\n\")\n",
    "cat(\"  5. account_period_data:\", nrow(account_period_data), \"accounts\\n\")\n",
    "cat(\"  6. surface_info:\", nrow(surface_info), \"surfaces (comprehensive account metadata)\\n\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 12: SAVE CLEANED DATASETS\n",
    "# ============================================================================\n",
    "\n",
    "cat(\"STEP 12: Saving cleaned datasets...\\n\")\n",
    "cat(\"-\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n",
    "\n",
    "if (!dir.exists(\"cleaned_data\")) {\n",
    "  dir.create(\"cleaned_data\")\n",
    "}\n",
    "\n",
    "timestamp <- format(Sys.time(), \"%Y%m%d_%H%M%S\")\n",
    "\n",
    "saveRDS(cleaned_data, paste0(\"cleaned_data/cleaned_posts_\", timestamp, \".rds\"))\n",
    "saveRDS(accounts, paste0(\"cleaned_data/accounts_summary_\", timestamp, \".rds\"))\n",
    "saveRDS(accounts_both_periods, paste0(\"cleaned_data/accounts_both_periods_\", timestamp, \".rds\"))\n",
    "saveRDS(data_both_periods, paste0(\"cleaned_data/posts_both_periods_\", timestamp, \".rds\"))\n",
    "saveRDS(weekly_data, paste0(\"cleaned_data/weekly_aggregation_\", timestamp, \".rds\"))\n",
    "saveRDS(monthly_data, paste0(\"cleaned_data/monthly_aggregation_\", timestamp, \".rds\"))\n",
    "saveRDS(account_period_data, paste0(\"cleaned_data/account_period_\", timestamp, \".rds\"))\n",
    "\n",
    "# Save surface info dataset (comprehensive account metadata)\n",
    "saveRDS(surface_info, paste0(\"cleaned_data/surface_info_\", timestamp, \".rds\"))\n",
    "cat(\"  ✓ surface_info_\", timestamp, \".rds\\n\", sep = \"\")\n",
    "\n",
    "# Save surface info as CSV for easy viewing\n",
    "write.csv(surface_info, \n",
    "          paste0(\"cleaned_data/surface_info_\", timestamp, \".csv\"),\n",
    "          row.names = FALSE, fileEncoding = \"UTF-8\")\n",
    "cat(\"  ✓ surface_info_\", timestamp, \".csv\\n\", sep = \"\")\n",
    "\n",
    "# Save surface IDs for MCL API queries (multiple formats for convenience)\n",
    "# Format 1: Simple text file with one ID per line\n",
    "writeLines(surface_ids_for_api$surface.id, \n",
    "           paste0(\"cleaned_data/surface_ids_for_api_\", timestamp, \".txt\"))\n",
    "cat(\"  ✓ surface_ids_for_api_\", timestamp, \".txt (one ID per line)\\n\", sep = \"\")\n",
    "\n",
    "# Format 2: CSV with ID, name, and list for reference\n",
    "write.csv(surface_ids_for_api,\n",
    "          paste0(\"cleaned_data/surface_ids_for_api_\", timestamp, \".csv\"),\n",
    "          row.names = FALSE, fileEncoding = \"UTF-8\")\n",
    "cat(\"  ✓ surface_ids_for_api_\", timestamp, \".csv\\n\", sep = \"\")\n",
    "\n",
    "# Format 3: R vector format (can be copy-pasted into MCL API script)\n",
    "r_vector_file <- paste0(\"cleaned_data/surface_ids_r_vector_\", timestamp, \".R\")\n",
    "r_vector_content <- paste0(\n",
    "  \"# Surface IDs for MCL API script\\n\",\n",
    "  \"# Generated: \", Sys.time(), \"\\n\",\n",
    "  \"# Total IDs: \", nrow(surface_ids_for_api), \"\\n\",\n",
    "  \"# Copy this vector to the 'account_ids' parameter in the MCL API script\\n\\n\",\n",
    "  \"account_ids <- c(\\n\",\n",
    "  paste0('  \"', surface_ids_for_api$surface.id, '\"', collapse = \",\\n\"),\n",
    "  \"\\n)\\n\"\n",
    ")\n",
    "writeLines(r_vector_content, r_vector_file)\n",
    "cat(\"  ✓ surface_ids_r_vector_\", timestamp, \".R (copy-paste ready)\\n\", sep = \"\")\n",
    "\n",
    "# Format 4: By main_list (separate files for targeted API queries)\n",
    "for (list_name in unique(surface_ids_for_api$main_list)) {\n",
    "  list_ids <- surface_ids_for_api %>% \n",
    "    filter(main_list == list_name) %>% \n",
    "    pull(surface.id)\n",
    "  \n",
    "  safe_list_name <- gsub(\"[^a-zA-Z0-9_]\", \"_\", list_name)\n",
    "  list_file <- paste0(\"cleaned_data/surface_ids_\", safe_list_name, \"_\", timestamp, \".txt\")\n",
    "  writeLines(list_ids, list_file)\n",
    "  cat(\"  ✓ surface_ids_\", safe_list_name, \"_\", timestamp, \".txt (\", \n",
    "      length(list_ids), \" IDs)\\n\", sep = \"\")\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "mp_groups <- unique(data$main_list[grepl(\"^MPs\", data$main_list)])\n",
    "total_views_imputed <- sum(data$views_imputed)\n",
    "pct_views_imputed <- round(100 * total_views_imputed / nrow(data), 2)\n",
    "\n",
    "# Calculate actual imputed values summary for metadata\n",
    "imputed_values_in_data <- data$statistics.views[data$views_imputed]\n",
    "\n",
    "metadata <- list(\n",
    "  timestamp = Sys.time(),\n",
    "  source_file = basename(data_file),\n",
    "  script_version = \"v3.2_group_specific_imputation_complete_weeks\",\n",
    "  dataset_version = if(length(mp_groups) > 1) \"v3.2_mp_split\" else \"v2_or_v3.1_single_mp\",\n",
    "  mp_groups = mp_groups,\n",
    "  all_main_lists = unique(data$main_list),\n",
    "  n_posts_original = n_before,\n",
    "  n_posts_cleaned = nrow(cleaned_data),\n",
    "  n_removed = n_removed,\n",
    "  n_accounts = nrow(accounts),\n",
    "  n_accounts_both_periods = nrow(accounts_both_periods),\n",
    "  n_surfaces = nrow(surface_info),\n",
    "  date_range = date_range,\n",
    "  policy_dates = policy_dates,\n",
    "  \n",
    "  study_timeframe = list(\n",
    "    start_date = study_start_date,\n",
    "    end_date = study_end_date,\n",
    "    description = \"Posts outside this range were excluded from analysis\"\n",
    "  ),\n",
    "  \n",
    "  complete_weeks = list(\n",
    "    enabled = TRUE,\n",
    "    first_complete_week_start = first_complete_week,\n",
    "    last_complete_week_end = last_complete_week_end,\n",
    "    n_total_weeks = n_total_weeks,\n",
    "    n_complete_weeks = n_complete_weeks,\n",
    "    n_partial_weeks_start = n_incomplete_start,\n",
    "    n_partial_weeks_end = n_incomplete_end,\n",
    "    posts_in_partial_weeks = posts_in_incomplete_weeks,\n",
    "    description = \"Weekly aggregation only includes complete weeks (Sunday to Saturday)\"\n",
    "  ),\n",
    "  \n",
    "  integrity_checks = list(\n",
    "    multi_list_accounts_found = !is.null(integrity_errors$multi_list_accounts),\n",
    "    duplicate_posts_found = !is.null(integrity_errors$duplicate_posts),\n",
    "    resolution_applied = if(has_critical_errors) multi_list_resolution else \"none_needed\",\n",
    "    warnings = names(integrity_warnings)\n",
    "  ),\n",
    "  \n",
    "  na_handling = list(\n",
    "    views = list(\n",
    "      method = \"Group-specific ratio-weighted\",\n",
    "      description = \"Power law fit to each group's near-threshold distribution (101-500 views)\",\n",
    "      configuration = list(\n",
    "        bin_width = IMPUTATION_BIN_WIDTH,\n",
    "        max_view = IMPUTATION_MAX_VIEW,\n",
    "        min_bins = IMPUTATION_MIN_BINS,\n",
    "        min_posts = IMPUTATION_MIN_POSTS,\n",
    "        fallback_ratio = POOLED_FALLBACK_RATIO\n",
    "      ),\n",
    "      seed = 42,\n",
    "      n_imputed = total_views_imputed,\n",
    "      pct_imputed = pct_views_imputed,\n",
    "      group_parameters = group_imputation_params,\n",
    "      imputation_log = imputation_log,\n",
    "      imputed_summary = list(\n",
    "        mean = round(mean(imputed_values_in_data), 1),\n",
    "        median = median(imputed_values_in_data),\n",
    "        sd = round(sd(imputed_values_in_data), 1),\n",
    "        pct_under_50 = round(100 * mean(imputed_values_in_data <= 50), 1)\n",
    "      ),\n",
    "      sensitivity = \"Max 0.002% difference between pooled and group-specific\",\n",
    "      by_group = na_views_by_group\n",
    "    ),\n",
    "    engagement = list(\n",
    "      method = \"REMOVAL\",\n",
    "      n_posts_removed = n_engagement_removed,\n",
    "      by_group = na_engagement_by_group\n",
    "    )\n",
    "  ),\n",
    "  \n",
    "  surface_info = list(\n",
    "    description = \"Comprehensive account/surface metadata extracted from posts\",\n",
    "    n_surfaces = nrow(surface_info),\n",
    "    columns = names(surface_info),\n",
    "    by_main_list = as.data.frame(surface_info %>% count(main_list)),\n",
    "    api_query_files = c(\n",
    "      paste0(\"surface_ids_for_api_\", timestamp, \".txt\"),\n",
    "      paste0(\"surface_ids_for_api_\", timestamp, \".csv\"),\n",
    "      paste0(\"surface_ids_r_vector_\", timestamp, \".R\")\n",
    "    )\n",
    "  ),\n",
    "  \n",
    "  data_flags = c(\"views_imputed\", \"views_pre_2017\", \"views_na_reason\")\n",
    ")\n",
    "\n",
    "saveRDS(metadata, paste0(\"cleaned_data/metadata_\", timestamp, \".rds\"))\n",
    "\n",
    "cat(\"✓ Saved all datasets to cleaned_data/\\n\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 13: SUMMARY REPORT\n",
    "# ============================================================================\n",
    "\n",
    "cat(\"=\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\")\n",
    "cat(\"DATA VALIDATION AND CLEANING SUMMARY (v3.2 - GROUP-SPECIFIC IMPUTATION)\\n\")\n",
    "cat(\"WITH COMPLETE WEEKS FILTERING\\n\")\n",
    "cat(\"=\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")\n",
    "\n",
    "cat(\"STUDY TIME-FRAME:\\n\")\n",
    "cat(\"  Start date:\", format(study_start_date, \"%Y-%m-%d\"), \"\\n\")\n",
    "cat(\"  End date:  \", format(study_end_date, \"%Y-%m-%d\"), \"\\n\")\n",
    "cat(\"  Posts outside this range were excluded\\n\\n\")\n",
    "\n",
    "cat(\"COMPLETE WEEKS FILTERING:\\n\")\n",
    "cat(\"  First complete week:\", format(first_complete_week, \"%Y-%m-%d\"), \"to\", \n",
    "    format(first_complete_week + days(6), \"%Y-%m-%d\"), \"\\n\")\n",
    "cat(\"  Last complete week: \", format(last_complete_week_start, \"%Y-%m-%d\"), \"to\", \n",
    "    format(last_complete_week_end, \"%Y-%m-%d\"), \"\\n\")\n",
    "cat(\"  Total complete weeks:\", n_complete_weeks, \"\\n\")\n",
    "cat(\"  Partial weeks excluded:\", n_incomplete_start + n_incomplete_end, \"\\n\")\n",
    "cat(\"  Posts in partial weeks:\", posts_in_incomplete_weeks, \n",
    "    sprintf(\"(%.2f%%)\\n\", 100 * posts_in_incomplete_weeks / nrow(cleaned_data)))\n",
    "cat(\"  NOTE: Partial week posts remain in cleaned_posts but excluded from weekly_aggregation\\n\\n\")\n",
    "\n",
    "cat(\"VIEW IMPUTATION METHOD (GROUP-SPECIFIC):\\n\")\n",
    "cat(\"  Method: Group-specific ratio-weighted based on power law extrapolation\\n\")\n",
    "cat(\"  Each group's parameters derived from its own near-threshold distribution\\n\\n\")\n",
    "cat(\"  Group-specific parameters:\\n\")\n",
    "for (g in names(group_imputation_params)) {\n",
    "  p <- group_imputation_params[[g]]\n",
    "  cat(sprintf(\"    %s: α=%.3f, ratio=%.2f → mean=%.1f, median=%d\\n\",\n",
    "              g, ifelse(is.na(p$alpha), NA, p$alpha), p$ratio, \n",
    "              p$expected_mean, p$expected_median))\n",
    "}\n",
    "cat(\"\\n\")\n",
    "cat(\"  Total imputed:\", total_views_imputed, sprintf(\"(%.1f%%)\\n\", pct_views_imputed))\n",
    "cat(\"  Sensitivity: Max 0.002% difference between pooled and group-specific\\n\\n\")\n",
    "\n",
    "cat(\"MERGE INTEGRITY:\\n\")\n",
    "if (has_critical_errors) {\n",
    "  cat(\"  ⚠ Issues found and resolved\\n\")\n",
    "  if (!is.null(integrity_errors$multi_list_accounts)) {\n",
    "    cat(\"    - Multi-list accounts: \", nrow(integrity_errors$multi_list_accounts), \n",
    "        \" (resolution: \", multi_list_resolution, \")\\n\", sep = \"\")\n",
    "  }\n",
    "  if (!is.null(integrity_errors$duplicate_posts)) {\n",
    "    cat(\"    - Duplicate posts: \", n_distinct(integrity_errors$duplicate_posts$id), \"\\n\", sep = \"\")\n",
    "  }\n",
    "} else {\n",
    "  cat(\"  ✓ All checks passed - no issues found\\n\")\n",
    "}\n",
    "cat(\"\\n\")\n",
    "\n",
    "cat(\"DATA CLEANING:\\n\")\n",
    "cat(\"  Original posts:\", n_before, \"\\n\")\n",
    "cat(\"  Posts removed (invalid + NA engagement):\", n_removed, \"\\n\")\n",
    "cat(\"  Final posts:\", nrow(cleaned_data), \"\\n\")\n",
    "cat(\"  Unique accounts:\", nrow(accounts), \"\\n\")\n",
    "cat(\"  Accounts in both periods:\", nrow(accounts_both_periods), \"\\n\")\n",
    "cat(\"  Surface info records:\", nrow(surface_info), \"\\n\\n\")\n",
    "\n",
    "cat(\"KEY VERIFICATION:\\n\")\n",
    "cat(\"  ✓ Each surface.id assigned to exactly ONE main_list\\n\")\n",
    "cat(\"  ✓ No duplicate post IDs\\n\")\n",
    "cat(\"  ✓ All posts within study time-frame (\", format(study_start_date, \"%Y-%m-%d\"), \n",
    "    \" to \", format(study_end_date, \"%Y-%m-%d\"), \")\\n\", sep = \"\")\n",
    "cat(\"  ✓ All engagement metrics are non-NA\\n\")\n",
    "cat(\"  ✓ Surface info dataset has one row per account\\n\")\n",
    "cat(\"  ✓ View imputation uses group-specific ratio-weighted method\\n\")\n",
    "cat(\"  ✓ Weekly aggregation includes only COMPLETE WEEKS\\n\\n\")\n",
    "\n",
    "cat(\"METHODS SECTION TEXT:\\n\")\n",
    "cat(\"-\" %>% rep(60) %>% paste0(collapse = \"\"), \"\\n\")\n",
    "cat(\"\n",
    "View counts at or below 100 are censored in the Meta Content Library API \n",
    "and returned as missing values. We imputed these censored values using a \n",
    "group-specific ratio-weighted approach. For each political group, we fit \n",
    "a power law model to bin counts in the 101-500 view range and extrapolated \n",
    "to estimate the distribution below the censoring threshold. This yielded \n",
    "group-specific parameters reflecting meaningful differences in how content \n",
    "falls below the visibility threshold. Extremists showed a left-skewed \n",
    "distribution (ratio ~1.65) indicating more very low-view content, while \n",
    "Prominent Politicians showed a right-skewed distribution (ratio ~0.66) \n",
    "suggesting their censored posts tend to be closer to the 100-view threshold. \n",
    "MPs fell between these extremes (ratio ~0.82-0.94). Sensitivity analysis \n",
    "confirmed that the maximum difference between group-specific and pooled \n",
    "imputation approaches was only 0.002%, indicating results are robust to \n",
    "imputation assumptions.\n",
    "\n",
    "For time series analysis, we aggregated posts into weekly intervals. To ensure \n",
    "consistent temporal comparison, only complete weeks (Sunday through Saturday) \n",
    "were included in the weekly aggregation. Partial weeks at the beginning and \n",
    "end of the study period were excluded from the weekly time series but remained \n",
    "in the post-level dataset for other analyses.\n",
    "\\n\")\n",
    "cat(\"-\" %>% rep(60) %>% paste0(collapse = \"\"), \"\\n\\n\")\n",
    "\n",
    "cat(\"=\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\")\n",
    "cat(\"✓ DATA VALIDATION AND CLEANING COMPLETE (v3.2 - COMPLETE WEEKS)\\n\")\n",
    "cat(\"=\" %>% rep(80) %>% paste0(collapse = \"\"), \"\\n\\n\")"
   ],
   "id": "7248716a-722d-44a5-bf8c-10424ae3c875",
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[NOTICE] 2 output(s) filtered out"
     ]
    }
   ]
  }
 ],
 "nbformat": 4
}